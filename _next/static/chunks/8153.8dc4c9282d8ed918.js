"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[8153,9824,1264],{80807:function(r,e,n){const t=n(14141).ZP.div.withConfig({componentId:"sc-8ce3e3c2-0"})`
  iframe {
    background-color: ${r=>{let{$hasSrcDoc:e}=r;return e?"#fff":"initial"}};
    border: 0;
    height: calc(100% - 36px - 33px);
    width: 100%;
  }

  nav {
    background-color: rgb(87, 87, 87);
    display: flex;
    padding: 4px 0;
    place-content: center;
    place-items: center;

    div {
      display: flex;
      justify-content: space-around;
      min-width: 102px;
      padding-left: 6px;
      width: 102px;
    }

    button {
      border-radius: 50%;
      display: flex;
      height: 28px;
      place-content: center;
      place-items: center;
      transition: background 0.2s ease-in-out;
      width: 28px;

      svg {
        fill: rgb(240, 240, 240);
        height: 22px;
        width: 22px;
      }

      &:hover {
        background-color: rgb(103, 103, 103);
      }

      &:active {
        background-color: rgb(110, 110, 110);
      }

      &:disabled {
        background-color: inherit;

        svg {
          fill: rgb(152, 152, 152);
        }
      }
    }

    &:not(:first-child) {
      border-bottom: 1px solid rgb(118, 115, 118);
      height: 33px;
      justify-content: left;
      padding: 0 8px;

      button {
        margin-bottom: 4px;
        margin-right: 4px;
      }
    }

    input {
      background-color: rgb(64, 62, 65);
      border-radius: 18px;
      color: rgb(255, 255, 255);
      font-family: ${r=>{let{theme:e}=r;return e.formats.systemFont}};
      font-size: 13px;
      height: 28px;
      letter-spacing: 0.2px;
      line-height: 26px;
      margin: 0 6px;
      padding: 0 13px;
      width: 100%;

      &:focus {
        outline: 2px solid rgb(138, 180, 248);
      }
    }
  }
`;e.Z=t},68153:function(r,e,n){n.r(e),n.d(e,{default:function(){return f}});var t=n(85893),o=n(59824),a=n(56758),i=n(26724),s=n(67294),c=n(34254),l=n(91264);const p=(r,e)=>{const n=(0,s.useCallback)(r,[r,...e]),[t,o]=(0,s.useState)(),[a,i]=(0,s.useState)();return(0,s.useEffect)((()=>{n().then((r=>{o(r),i(void 0)})).catch((r=>{o(void 0),i(r)}))}),[n]),[t,a]},m=r=>{const{readFile:e}=(0,i.o)(),[n,t]=p((async()=>{if(r)return e(r)}),[r,e]);return[n,t]},{runtime:d}=o.defaultRuntimeConfig,u=new TextDecoder;var f=r=>{const e=`\n    ${d}\n    // lockdown()\n      // START BUNDLE RUNTIME ================================\r\n  const { loadApplication } = (function(){\r\n  'use strict';\r\nreturn (() => {\r\n  const functors = [\r\n// === functors[0] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n/** @typedef {__import__('./types.js').AttenuationDefinition} AttenuationDefinition */\r\n/** @typedef {__import__('./types.js').UnifiedAttenuationDefinition} UnifiedAttenuationDefinition */\r\n\r\nconst { entries, keys}=   Object;\r\nconst { isArray}=   Array;\r\nconst q=  JSON.stringify;\r\n\r\nconst ATTENUATOR_KEY=  'attenuate';\r\nconst ATTENUATOR_PARAMS_KEY=  'params';\r\nconst WILDCARD_POLICY_VALUE=  'any';\r\nconst POLICY_FIELDS_LOOKUP=  ['builtins', 'globals', 'packages'];\r\n\r\n/**\r\n *\r\n * @param {object} packagePolicy\r\n * @param {string} field\r\n * @param {string} itemName\r\n * @returns {boolean | object}\r\n */\r\nconst        policyLookupHelper=  (packagePolicy, field, itemName)=>  {\r\n  if( !POLICY_FIELDS_LOOKUP.includes(field)) {\r\n    throw Error( \`Invalid field \${q(field)}\`);\r\n   }\r\n  if(\r\n    typeof packagePolicy!==  'object'||\r\n    packagePolicy===  null||\r\n    !packagePolicy[field])\r\n    {\r\n    return false;\r\n   }\r\n\r\n  if( packagePolicy[field]===  WILDCARD_POLICY_VALUE) {\r\n    return true;\r\n   }\r\n  if( packagePolicy[field][itemName]) {\r\n    return packagePolicy[field][itemName];\r\n   }\r\n  return false;\r\n };\r\n\r\n/**\r\n * Checks if the policy value is set to wildcard to allow everything\r\n *\r\n * @param {*} policyValue\r\n * @returns {boolean}\r\n */$h‍_once.policyLookupHelper(policyLookupHelper);\r\nconst        isAllowingEverything=  (policyValue)=>\r\n  policyValue===  WILDCARD_POLICY_VALUE;\r\n\r\n/**\r\n *\r\n * @param {AttenuationDefinition} potentialDefinition\r\n * @returns {boolean}\r\n */$h‍_once.isAllowingEverything(isAllowingEverything);\r\nconst        isAttenuationDefinition=  (potentialDefinition)=>{\r\n  return(\r\n     typeof potentialDefinition===  'object'&&\r\n      typeof potentialDefinition[ATTENUATOR_KEY]===  'string'||   // object with attenuator name\r\n    isArray(potentialDefinition) // params for default attenuator\r\n);\r\n };\r\n\r\n/**\r\n *\r\n * @param {AttenuationDefinition} attenuationDefinition\r\n * @returns {UnifiedAttenuationDefinition}\r\n */$h‍_once.isAttenuationDefinition(isAttenuationDefinition);\r\nconst        getAttenuatorFromDefinition=  (attenuationDefinition)=>{\r\n  if( !isAttenuationDefinition(attenuationDefinition)) {\r\n    throw Error(\r\n       \`Invalid attenuation \${q(\r\n        attenuationDefinition)\r\n        }, must be an array of params for default attenuator or an object with an attenuator key\`);\r\n\r\n   }\r\n  if( isArray(attenuationDefinition)) {\r\n    return {\r\n      displayName: '<default attenuator>',\r\n      specifier: null,\r\n      params: attenuationDefinition};\r\n\r\n   }else {\r\n    return {\r\n      displayName: attenuationDefinition[ATTENUATOR_KEY],\r\n      specifier: attenuationDefinition[ATTENUATOR_KEY],\r\n      params: attenuationDefinition[ATTENUATOR_PARAMS_KEY]};\r\n\r\n   }\r\n };$h‍_once.getAttenuatorFromDefinition(getAttenuatorFromDefinition);\r\n\r\nconst isRecordOf=  (item, predicate)=>  {\r\n  if( typeof item!==  'object'||  item===  null||  isArray(item)) {\r\n    return false;\r\n   }\r\n  return entries(item).every(([key, value])=>  predicate(value, key));\r\n };\r\nconst isBoolean=  (item)=>typeof item===  'boolean';\r\nconst predicateOr=\r\n  (...predicates)=>\r\n  (item)=>\r\n    predicates.some((p)=>p(item));\r\nconst isPolicyItem=  (item)=>\r\n  item===  undefined||\r\n  item===  WILDCARD_POLICY_VALUE||\r\n  isRecordOf(item, isBoolean);\r\n\r\n/**\r\n *\r\n * @param {unknown} allegedPackagePolicy\r\n * @param {string} path\r\n * @param {string} [url]\r\n * @returns {void}\r\n */\r\nconst        assertPackagePolicy=  (allegedPackagePolicy, path, url)=>  {\r\n  if( allegedPackagePolicy===  undefined) {\r\n    return;\r\n   }\r\n  const inUrl=  url?   \` in \${q(url)}\`: '';\r\n\r\n  const packagePolicy=  Object(allegedPackagePolicy);\r\n  assert(\r\n    allegedPackagePolicy===  packagePolicy&&  !isArray(allegedPackagePolicy),\r\n     \`\${path} must be an object, got \${allegedPackagePolicy}\${inUrl}\`);\r\n\r\n  const {\r\n    packages,\r\n    builtins,\r\n    globals,\r\n    noGlobalFreeze,\r\n    defaultAttenuator: _ignore, // a carve out for the default attenuator in compartment map\r\n    ...extra}=\r\n      packagePolicy;\r\n\r\n  assert(\r\n    keys(extra).length===  0,\r\n     \`\${path} must not have extra properties, got \${q(keys(extra))}\${inUrl}\`);\r\n\r\n\r\n  assert(\r\n    noGlobalFreeze===  undefined||  typeof noGlobalFreeze===  'boolean',\r\n     \`\${path}.noGlobalFreeze must be a boolean, got \${q({\r\n      noGlobalFreeze})\r\n       }\${inUrl}\`);\r\n\r\n\r\n  isPolicyItem(packages)||\r\n    assert.fail(\r\n       \`\${path}.packages must be a record of booleans, got \${q({\r\n        packages})\r\n         }\${inUrl}\`);\r\n\r\n\r\n  isPolicyItem(globals)||\r\n    isAttenuationDefinition(globals)||\r\n    assert.fail(\r\n       \`\${path}.globals must be a record of booleans or a single attenuation, got \${q(\r\n        {\r\n          globals})\r\n\r\n        }\${inUrl}\`);\r\n\r\n\r\n  isPolicyItem(builtins)||\r\n    isRecordOf(builtins, predicateOr(isBoolean, isAttenuationDefinition))||\r\n    assert.fail(\r\n       \`\${path}.builtins must be a record of booleans or attenuations, got \${q({\r\n        builtins})\r\n         }\${inUrl}\`);\r\n\r\n };\r\n\r\n/**\r\n *\r\n * @param {unknown} allegedPolicy\r\n * @returns {void}\r\n */$h‍_once.assertPackagePolicy(assertPackagePolicy);\r\nconst        assertPolicy=  (allegedPolicy)=>{\r\n  if( allegedPolicy===  undefined) {\r\n    return;\r\n   }\r\n  const policy=  Object(allegedPolicy);\r\n  assert(\r\n    allegedPolicy===  policy&&  !Array.isArray(policy),\r\n     \`policy must be an object, got \${allegedPolicy}\`);\r\n\r\n\r\n  const { resources, entry, defaultAttenuator, ...extra}=   policy;\r\n  assert(\r\n    keys(extra).length===  0,\r\n     \`policy must not have extra properties, got \${q(keys(extra))}\`);\r\n\r\n\r\n  assert(\r\n    typeof resources===  'object'&&  resources!==  null,\r\n     \`policy.resources must be an object, got \${q(resources)}\`);\r\n\r\n  assert(\r\n    !defaultAttenuator||  typeof defaultAttenuator===  'string',\r\n     \`policy.defaultAttenuator must be a string, got \${q(resources)}\`);\r\n\r\n\r\n  assertPackagePolicy(entry,  \`policy.entry\`);\r\n\r\n  entries(resources).forEach(([key, value])=>  {\r\n    assertPackagePolicy(value,  \`policy.resources["\${key}"]\`);\r\n   });\r\n };$h‍_once.assertPolicy(assertPolicy);\r\n})\r\n\r\n,// === functors[1] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let assertPackagePolicy;$h‍_imports([["./policy-format.js", [["assertPackagePolicy", [$h‍_a => (assertPackagePolicy = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n\r\n// TODO convert to the new \`||\` assert style.\r\n// Deferred because this file pervasively uses simple template strings rather than\r\n// template strings tagged with \`assert.details\` (aka \`X\`), and uses\r\n// this definition of \`q\` rather than \`assert.quote\`\r\nconst q=  JSON.stringify;\r\n\r\nconst moduleLanguages=  [\r\n  'cjs',\r\n  'mjs',\r\n  'json',\r\n  'text',\r\n  'bytes',\r\n  'pre-mjs-json',\r\n  'pre-cjs-json'];\r\n\r\n\r\n/** @type {(a: string, b: string) => number} */\r\n// eslint-disable-next-line no-nested-ternary\r\nconst        stringCompare=  (a, b)=>   a===  b?  0:  a<  b?  -1:  1;\r\n\r\n/**\r\n * @param {number} length\r\n * @param {string} term\r\n */$h‍_once.stringCompare(stringCompare);\r\nconst cumulativeLength=  (length, term)=>  {\r\n  return length+  term.length;\r\n };\r\n\r\n/**\r\n * @param {Array<string> | undefined} a\r\n * @param {Array<string> | undefined} b\r\n */\r\nconst        pathCompare=  (a, b)=>  {\r\n  // Undefined is not preferred\r\n  if( a===  undefined&&  b===  undefined) {\r\n    return 0;\r\n   }\r\n  if( a===  undefined) {\r\n    return 1;\r\n   }\r\n  if( b===  undefined) {\r\n    return -1;\r\n   }\r\n  // Prefer the shortest dependency path.\r\n  if( a.length!==  b.length) {\r\n    return a.length-  b.length;\r\n   }\r\n  // Otherwise, favor the shortest cumulative length.\r\n  const aSum=  a.reduce(cumulativeLength, 0);\r\n  const bSum=  b.reduce(cumulativeLength, 0);\r\n  if( aSum!==  bSum) {\r\n    return aSum-  bSum;\r\n   }\r\n  // Otherwise, compare terms lexically.\r\n  assert(a.length===  b.length); // Reminder\r\n  // This loop guarantees that if any pair of terms is different, including the\r\n  // case where one is a prefix of the other, we will return a non-zero value.\r\n  for( let i=  0; i<  a.length; i+=  1) {\r\n    const comparison=  stringCompare(a[i], b[i]);\r\n    if( comparison!==  0) {\r\n      return comparison;\r\n     }\r\n   }\r\n  // If all pairs of terms are the same respective lengths, we are guaranteed\r\n  // that they are exactly the same or one of them is lexically distinct and would\r\n  // have already been caught.\r\n  return 0;\r\n };\r\n\r\n/**\r\n * @template T\r\n * @param {Iterable<T>} iterable\r\n */$h‍_once.pathCompare(pathCompare);\r\nfunction* enumerate(iterable) {\r\n  let index=  0;\r\n  for( const value of iterable) {\r\n    yield [index, value];\r\n    index+=  1;\r\n   }\r\n }\r\n\r\n/**\r\n * @param {Record<string, unknown>} object\r\n * @param {string} message\r\n */\r\nconst assertEmptyObject=  (object, message)=>  {\r\n  assert(Object.keys(object).length===  0, message);\r\n };\r\n\r\n/**\r\n * @param {unknown} tags\r\n * @param {string} url\r\n */\r\nconst assertTags=  (tags, url)=>  {\r\n  if( tags===  undefined) return;\r\n  assert(\r\n    Array.isArray(tags),\r\n     \`tags must be an array, got \${tags} in \${q(url)}\`);\r\n\r\n  for( const [index, value]of  enumerate(tags)) {\r\n    assert.typeof(\r\n      value,\r\n      'string',\r\n       \`tags[\${index}] must be a string, got \${value} in \${q(url)}\`);\r\n\r\n   }\r\n };\r\n\r\n/**\r\n * @param {Record<string, unknown>} allegedModule\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertCompartmentModule=  (allegedModule, path, url)=>  {\r\n  const { compartment, module, ...extra}=   allegedModule;\r\n  assertEmptyObject(\r\n    extra,\r\n     \`\${path} must not have extra properties, got \${q({\r\n      extra,\r\n      compartment})\r\n       } in \${q(url)}\`);\r\n\r\n  assert.typeof(\r\n    compartment,\r\n    'string',\r\n     \`\${path}.compartment must be a string, got \${q(compartment)} in \${q(url)}\`);\r\n\r\n  assert.typeof(\r\n    module,\r\n    'string',\r\n     \`\${path}.module must be a string, got \${q(module)} in \${q(url)}\`);\r\n\r\n };\r\n\r\n/**\r\n * @param {Record<string, unknown>} allegedModule\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertFileModule=  (allegedModule, path, url)=>  {\r\n  const { location, parser, sha512, ...extra}=   allegedModule;\r\n  assertEmptyObject(\r\n    extra,\r\n     \`\${path} must not have extra properties, got \${q(\r\n      Object.keys(extra))\r\n      } in \${q(url)}\`);\r\n\r\n  assert.typeof(\r\n    location,\r\n    'string',\r\n     \`\${path}.location must be a string, got \${q(location)} in \${q(url)}\`);\r\n\r\n  assert.typeof(\r\n    parser,\r\n    'string',\r\n     \`\${path}.parser must be a string, got \${q(parser)} in \${q(url)}\`);\r\n\r\n  assert(\r\n    moduleLanguages.includes(parser),\r\n     \`\${path}.parser must be one of \${q(moduleLanguages)}, got \${parser} in \${q(\r\n      url)\r\n      }\`);\r\n\r\n\r\n  if( sha512!==  undefined) {\r\n    assert.typeof(\r\n      sha512,\r\n      'string',\r\n       \`\${path}.sha512 must be a string, got \${q(sha512)} in \${q(url)}\`);\r\n\r\n   }\r\n };\r\n\r\n/**\r\n * @param {Record<string, unknown>} allegedModule\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertExitModule=  (allegedModule, path, url)=>  {\r\n  const { exit, ...extra}=   allegedModule;\r\n  assertEmptyObject(\r\n    extra,\r\n     \`\${path} must not have extra properties, got \${q(\r\n      Object.keys(extra))\r\n      } in \${q(url)}\`);\r\n\r\n  assert.typeof(\r\n    exit,\r\n    'string',\r\n     \`\${path}.exit must be a string, got \${q(exit)} in \${q(url)}\`);\r\n\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedModule\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertModule=  (allegedModule, path, url)=>  {\r\n  const moduleDescriptor=  Object(allegedModule);\r\n  assert(\r\n    allegedModule===  moduleDescriptor&&  !Array.isArray(moduleDescriptor),\r\n     \`\${path} must be an object, got \${allegedModule} in \${q(url)}\`);\r\n\r\n\r\n  const { compartment, module, location, parser, exit, deferredError}=\r\n    moduleDescriptor;\r\n  if( compartment!==  undefined||  module!==  undefined) {\r\n    assertCompartmentModule(moduleDescriptor, path, url);\r\n   }else if( location!==  undefined||  parser!==  undefined) {\r\n    assertFileModule(moduleDescriptor, path, url);\r\n   }else if( exit!==  undefined) {\r\n    assertExitModule(moduleDescriptor, path, url);\r\n   }else if( deferredError!==  undefined) {\r\n    assert.typeof(\r\n      deferredError,\r\n      'string',\r\n       \`\${path}.deferredError must be a string contaiing an error message\`);\r\n\r\n   }else {\r\n    assert.fail(\r\n       \`\${path} is not a valid module descriptor, got \${q(allegedModule)} in \${q(\r\n        url)\r\n        }\`);\r\n\r\n   }\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedModules\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertModules=  (allegedModules, path, url)=>  {\r\n  const modules=  Object(allegedModules);\r\n  assert(\r\n    allegedModules===  modules||  !Array.isArray(modules),\r\n     \`modules must be an object, got \${q(allegedModules)} in \${q(url)}\`);\r\n\r\n  for( const [key, value]of  Object.entries(modules)) {\r\n    assertModule(value,  \`\${path}.modules[\${q(key)}]\`,url);\r\n   }\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedParsers\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertParsers=  (allegedParsers, path, url)=>  {\r\n  if( allegedParsers===  undefined) {\r\n    return;\r\n   }\r\n  const parsers=  Object(allegedParsers);\r\n  assert(\r\n    allegedParsers===  parsers&&  !Array.isArray(parsers),\r\n     \`\${path}.parsers must be an object, got \${allegedParsers} in \${q(url)}\`);\r\n\r\n\r\n  for( const [key, value]of  Object.entries(parsers)) {\r\n    assert.typeof(\r\n      key,\r\n      'string',\r\n       \`all keys of \${path}.parsers must be strings, got \${key} in \${q(url)}\`);\r\n\r\n    assert.typeof(\r\n      value,\r\n      'string',\r\n       \`\${path}.parsers[\${q(key)}] must be a string, got \${value} in \${q(url)}\`);\r\n\r\n    assert(\r\n      moduleLanguages.includes(value),\r\n       \`\${path}.parsers[\${q(key)}] must be one of \${q(\r\n        moduleLanguages)\r\n        }, got \${value} in \${q(url)}\`);\r\n\r\n   }\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedScope\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertScope=  (allegedScope, path, url)=>  {\r\n  const scope=  Object(allegedScope);\r\n  assert(\r\n    allegedScope===  scope&&  !Array.isArray(scope),\r\n     \`\${path} must be an object, got \${allegedScope} in \${q(url)}\`);\r\n\r\n\r\n  const { compartment, ...extra}=   scope;\r\n  assertEmptyObject(\r\n    extra,\r\n     \`\${path} must not have extra properties, got \${q(\r\n      Object.keys(extra))\r\n      } in \${q(url)}\`);\r\n\r\n\r\n  assert.typeof(\r\n    compartment,\r\n    'string',\r\n     \`\${path}.compartment must be a string, got \${q(compartment)} in \${q(url)}\`);\r\n\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedScopes\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertScopes=  (allegedScopes, path, url)=>  {\r\n  if( allegedScopes===  undefined) {\r\n    return;\r\n   }\r\n  const scopes=  Object(allegedScopes);\r\n  assert(\r\n    allegedScopes===  scopes&&  !Array.isArray(scopes),\r\n     \`\${path}.scopes must be an object, got \${q(allegedScopes)} in \${q(url)}\`);\r\n\r\n\r\n  for( const [key, value]of  Object.entries(scopes)) {\r\n    assert.typeof(\r\n      key,\r\n      'string',\r\n       \`all keys of \${path}.scopes must be strings, got \${key} in \${q(url)}\`);\r\n\r\n    assertScope(value,  \`\${path}.scopes[\${q(key)}]\`,url);\r\n   }\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedTypes\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertTypes=  (allegedTypes, path, url)=>  {\r\n  if( allegedTypes===  undefined) {\r\n    return;\r\n   }\r\n  const types=  Object(allegedTypes);\r\n  assert(\r\n    allegedTypes===  types&&  !Array.isArray(types),\r\n     \`\${path}.types must be an object, got \${allegedTypes} in \${q(url)}\`);\r\n\r\n\r\n  for( const [key, value]of  Object.entries(types)) {\r\n    assert.typeof(\r\n      key,\r\n      'string',\r\n       \`all keys of \${path}.types must be strings, got \${key} in \${q(url)}\`);\r\n\r\n    assert.typeof(\r\n      value,\r\n      'string',\r\n       \`\${path}.types[\${q(key)}] must be a string, got \${value} in \${q(url)}\`);\r\n\r\n    assert(\r\n      moduleLanguages.includes(value),\r\n       \`\${path}.types[\${q(key)}] must be one of \${q(\r\n        moduleLanguages)\r\n        }, got \${value} in \${q(url)}\`);\r\n\r\n   }\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedPolicy\r\n * @param {string} path\r\n * @param {string} [url]\r\n */\r\n\r\nconst assertPolicy=  (\r\n  allegedPolicy,\r\n  path,\r\n  url=  '<unknown-compartment-map.json>')=>\r\n     {\r\n  assertPackagePolicy(allegedPolicy,  \`\${path}.policy\`,url);\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedCompartment\r\n * @param {string} path\r\n * @param {string} url\r\n */\r\nconst assertCompartment=  (allegedCompartment, path, url)=>  {\r\n  const compartment=  Object(allegedCompartment);\r\n  assert(\r\n    allegedCompartment===  compartment&&  !Array.isArray(compartment),\r\n     \`\${path} must be an object, got \${allegedCompartment} in \${q(url)}\`);\r\n\r\n\r\n  const {\r\n    location,\r\n    name,\r\n    label,\r\n    parsers,\r\n    types,\r\n    scopes,\r\n    modules,\r\n    policy,\r\n    ...extra}=\r\n      compartment;\r\n\r\n  assertEmptyObject(\r\n    extra,\r\n     \`\${path} must not have extra properties, got \${q(\r\n      Object.keys(extra))\r\n      } in \${q(url)}\`);\r\n\r\n\r\n  assert.typeof(\r\n    location,\r\n    'string',\r\n     \`\${path}.location in \${q(url)} must be string, got \${q(location)}\`);\r\n\r\n  assert.typeof(\r\n    name,\r\n    'string',\r\n     \`\${path}.name in \${q(url)} must be string, got \${q(name)}\`);\r\n\r\n  assert.typeof(\r\n    label,\r\n    'string',\r\n     \`\${path}.label in \${q(url)} must be string, got \${q(label)}\`);\r\n\r\n\r\n  assertModules(modules, path, url);\r\n  assertParsers(parsers, path, url);\r\n  assertScopes(scopes, path, url);\r\n  assertTypes(types, path, url);\r\n  assertPolicy(policy, path, url);\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedCompartments\r\n * @param {string} url\r\n */\r\nconst assertCompartments=  (allegedCompartments, url)=>  {\r\n  const compartments=  Object(allegedCompartments);\r\n  assert(\r\n    allegedCompartments===  compartments||  !Array.isArray(compartments),\r\n     \`compartments must be an object, got \${q(allegedCompartments)} in \${q(\r\n      url)\r\n      }\`);\r\n\r\n  for( const [key, value]of  Object.entries(compartments)) {\r\n    assertCompartment(value,  \`compartments[\${q(key)}]\`,url);\r\n   }\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedEntry\r\n * @param {string} url\r\n */\r\nconst assertEntry=  (allegedEntry, url)=>  {\r\n  const entry=  Object(allegedEntry);\r\n  assert(\r\n    allegedEntry===  entry&&  !Array.isArray(entry),\r\n     \`"entry" must be an object in compartment map, got \${allegedEntry} in \${q(\r\n      url)\r\n      }\`);\r\n\r\n  const { compartment, module, ...extra}=   entry;\r\n  assertEmptyObject(\r\n    extra,\r\n     \`"entry" must not have extra properties in compartment map, got \${q(\r\n      Object.keys(extra))\r\n      } in \${q(url)}\`);\r\n\r\n  assert.typeof(\r\n    compartment,\r\n    'string',\r\n     \`entry.compartment must be a string in compartment map, got \${compartment} in \${q(\r\n      url)\r\n      }\`);\r\n\r\n  assert.typeof(\r\n    module,\r\n    'string',\r\n     \`entry.module must be a string in compartment map, got \${module} in \${q(\r\n      url)\r\n      }\`);\r\n\r\n };\r\n\r\n/**\r\n * @param {unknown} allegedCompartmentMap\r\n * @param {string} [url]\r\n * @returns {asserts compartmentMap is __import__('./types.js').CompartmentMapDescriptor}\r\n */\r\n\r\nconst        assertCompartmentMap=  (\r\n  allegedCompartmentMap,\r\n  url=  '<unknown-compartment-map.json>')=>\r\n     {\r\n  const compartmentMap=  Object(allegedCompartmentMap);\r\n  assert(\r\n    allegedCompartmentMap===  compartmentMap&&  !Array.isArray(compartmentMap),\r\n     \`Compartment map must be an object, got \${allegedCompartmentMap} in \${q(\r\n      url)\r\n      }\`);\r\n\r\n  const { tags, entry, compartments, ...extra}=   Object(compartmentMap);\r\n  assertEmptyObject(\r\n    extra,\r\n     \`Compartment map must not have extra properties, got \${q(\r\n      Object.keys(extra))\r\n      } in \${q(url)}\`);\r\n\r\n  assertTags(tags, url);\r\n  assertEntry(entry, url);\r\n  assertCompartments(compartments, url);\r\n };$h‍_once.assertCompartmentMap(assertCompartmentMap);\r\n})\r\n\r\n,// === functors[2] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n/**\r\n * Parses JSON and, if necessary, throws exceptions that include the location\r\n * of the offending file.\r\n *\r\n * @param {string} source\r\n * @param {string} location\r\n */\r\nconst        parseLocatedJson=  (source, location)=>  {\r\n  try {\r\n    return JSON.parse(source);\r\n   }catch( error) {\r\n    if( error instanceof SyntaxError) {\r\n      throw new SyntaxError( \`Cannot parse JSON from \${location}, \${error}\`);\r\n     }\r\n    throw error;\r\n   }\r\n };$h‍_once.parseLocatedJson(parseLocatedJson);\r\n})\r\n\r\n,// === functors[3] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n/**\r\n * \`parseExtension\` returns the file extension for the given URL, or an empty\r\n * string if the path has no extension.\r\n * Exported for tests.\r\n *\r\n * @param {string} location\r\n * @returns {string}\r\n */\r\nconst        parseExtension=  (location)=>{\r\n  const lastSlash=  location.lastIndexOf('/');\r\n  if( lastSlash<  0) {\r\n    return '';\r\n   }\r\n  const base=  location.slice(lastSlash+  1);\r\n  const lastDot=  base.lastIndexOf('.');\r\n  if( lastDot<  0) {\r\n    return '';\r\n   }\r\n  return base.slice(lastDot+  1);\r\n };$h‍_once.parseExtension(parseExtension);\r\n})\r\n\r\n,// === functors[4] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n// q, as in quote, for error messages.\r\nconst q=  JSON.stringify;\r\n\r\n/**\r\n * Advances a partial module specifier solution by following the path\r\n * components in the given problem.\r\n * The problem may not produce a path that escapes the solution, that is, the\r\n * problem may not traverse up from an empty solution.\r\n * \`Solve\` returns false if the problem attempts to escape.\r\n * Advanding a partial solution is the core of \`resolve\`, \`join\`, and\r\n * \`relativize\`, which have different invariants.\r\n *\r\n * @param {Array<string>} solution - fully resolved path components, including\r\n * any from a prior path resolution initially.\r\n * @param {Array<string>} problem - partially resolved path components, that\r\n * is, including '.' and '..' components.\r\n * @returns {boolean} whether the solver terminated early because of a\r\n * nonsensical attempt to traverse above the root directory.\r\n */\r\nconst solve=  (solution, problem)=>  {\r\n  for( const part of problem) {\r\n    if( part===  '.'||  part===  '') {\r\n      // no-op\r\n     }else if( part===  '..') {\r\n      if( solution.length===  0) {\r\n        return false;\r\n       }\r\n      solution.pop();\r\n     }else {\r\n      solution.push(part);\r\n     }\r\n   }\r\n  return true;\r\n };\r\n\r\n/**\r\n * \`Resolve\` computes the full module specifier for a given imported module specifier\r\n * relative to the referrer module specifier.\r\n * In Node.js compartments, the referrer must be an internal module specifier\r\n * in the context of a compartment, and all internal module specifiers begin\r\n * with a "." path component.\r\n * The referent may be either internal or external.\r\n * In Node.js, fully resolved paths are valid module specifiers, but these\r\n * paths that begin with / are disallowed as they could be used to defeat\r\n * compartment containment.\r\n *\r\n * @param {string} spec - a path to resolve.\r\n * @param {string} referrer - the fully resolved path of referrer module.\r\n * @returns {string} the fully resolved path.\r\n */\r\nconst        resolve=  (spec, referrer)=>  {\r\n  spec=  String(spec||  '');\r\n  referrer=  String(referrer||  '');\r\n\r\n  if( spec.startsWith('/')) {\r\n    throw new Error( \`Module specifier \${q(spec)} must not begin with "/"\`);\r\n   }\r\n  if( !referrer.startsWith('./')) {\r\n    throw new Error( \`Module referrer \${q(referrer)} must begin with "./"\`);\r\n   }\r\n\r\n  const specParts=  spec.split('/');\r\n  const solution=  [];\r\n  const problem=  [];\r\n  if( specParts[0]===  '.'||  specParts[0]===  '..') {\r\n    const referrerParts=  referrer.split('/');\r\n    problem.push(...referrerParts);\r\n    problem.pop();\r\n    solution.push('.');\r\n   }\r\n  problem.push(...specParts);\r\n\r\n  if( !solve(solution, problem)) {\r\n    throw new Error(\r\n       \`Module specifier \${q(spec)} via referrer \${q(\r\n        referrer)\r\n        } must not traverse behind an empty path\`);\r\n\r\n   }\r\n\r\n  return solution.join('/');\r\n };\r\n\r\n/**\r\n * To construct a module map from a node_modules package, inter-package linkage\r\n * requires connecting a full base module specifier like "dependency-package"\r\n * to the other package's full internal module specifier like "." or\r\n * "./utility", to form a local full module specifier like "dependency-package"\r\n * or "dependency-package/utility".\r\n * This type of join may assert that the base is absolute and the referrent is\r\n * relative.\r\n *\r\n * @param {string} base - the fully resolved path of a module.\r\n * @param {string} spec - the partially resolved path of another module.\r\n * @returns {string} the fully resolved path of the specified module.\r\n */$h‍_once.resolve(resolve);\r\nconst        join=  (base, spec)=>  {\r\n  spec=  String(spec||  '');\r\n  base=  String(base||  '');\r\n\r\n  const specParts=  spec.split('/');\r\n  const baseParts=  base.split('/');\r\n\r\n  if( specParts.length>  1&&  specParts[0]===  '') {\r\n    throw new Error( \`Module specifier \${q(spec)} must not start with "/"\`);\r\n   }\r\n  if( baseParts[0]===  '.'||  baseParts[0]===  '..') {\r\n    throw new Error( \`External module specifier \${q(base)} must be absolute\`);\r\n   }\r\n  if( specParts[0]!==  '.') {\r\n    throw new Error( \`Internal module specifier \${q(spec)} must be relative\`);\r\n   }\r\n\r\n  const solution=  [];\r\n  if( !solve(solution, specParts)) {\r\n    throw new Error(\r\n       \`Module specifier \${q(spec)} via base \${q(\r\n        base)\r\n        } must not refer to a module outside of the base\`);\r\n\r\n   }\r\n\r\n  return [base, ...solution].join('/');\r\n };\r\n\r\n/**\r\n * Relativize turns absolute identifiers into relative identifiers.\r\n * In package.json, internal module identifiers can be either relative or\r\n * absolute, but compartments backed by node_modules always use relative module\r\n * specifiers for internal linkage.\r\n *\r\n * @param {string} spec - a module specifier that of a local module, that might\r\n * be erroneously framed without an initial '.' path component.\r\n * @returns {string} the idempotent module specifier, ensured to begin with\r\n * '.'.\r\n */$h‍_once.join(join);\r\nconst        relativize=  (spec)=>{\r\n  spec=  String(spec||  '');\r\n\r\n  const solution=  [];\r\n  if( !solve(solution, spec.split('/'))) {\r\n    throw Error(\r\n       \`Module specifier \${q(spec)} must not traverse behind an empty path\`);\r\n\r\n   }\r\n\r\n  return ['.', ...solution].join('/');\r\n };$h‍_once.relativize(relativize);\r\n})\r\n\r\n,// === functors[5] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let policyLookupHelper,isAttenuationDefinition,getAttenuatorFromDefinition,isAllowingEverything;$h‍_imports([["./policy-format.js", [["policyLookupHelper", [$h‍_a => (policyLookupHelper = $h‍_a)]],["isAttenuationDefinition", [$h‍_a => (isAttenuationDefinition = $h‍_a)]],["getAttenuatorFromDefinition", [$h‍_a => (getAttenuatorFromDefinition = $h‍_a)]],["isAllowingEverything", [$h‍_a => (isAllowingEverything = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nconst { entries, values, assign, keys, freeze}=   Object;\r\nconst q=  JSON.stringify;\r\n\r\n/**\r\n * Const string to identify the internal attenuators compartment\r\n */\r\nconst        ATTENUATORS_COMPARTMENT=  '<ATTENUATORS>';\r\n\r\n/**\r\n * Copies properties (optionally limited to a specific list) from one object to another.\r\n *\r\n * @param {object} from\r\n * @param {object} to\r\n * @param {Array<string | symbol>} [list]\r\n * @returns {object}\r\n */$h‍_once.ATTENUATORS_COMPARTMENT(ATTENUATORS_COMPARTMENT);\r\nconst selectiveCopy=  (from, to, list)=>  {\r\n  if( !list) {\r\n    list=  keys(from);\r\n   }\r\n  for( let index=  0; index<  list.length; index+=  1) {\r\n    const key=  list[index];\r\n    // If an endowment is missing, global value is undefined.\r\n    // This is an expected behavior if globals are used for platform feature detection\r\n    to[key]=  from[key];\r\n   }\r\n  return to;\r\n };\r\n\r\nconst collectAttenuators=  (attenuators, policyFragment)=>  {\r\n  if( policyFragment.attenuate) {\r\n    attenuators.push(policyFragment.attenuate);\r\n   }\r\n  for( const value of values(policyFragment)) {\r\n    if( typeof value===  'object'&&  value!==  null) {\r\n      collectAttenuators(attenuators, value);\r\n     }\r\n   }\r\n };\r\n\r\nconst attenuatorsCache=  new WeakMap();\r\n/**\r\n * Goes through policy and lists all attenuator specifiers used.\r\n * Memoization keyed on policy object reference\r\n *\r\n * @param {object} policy\r\n * @returns {Array<string>} attenuators\r\n */\r\nconst        detectAttenuators=  (policy)=>{\r\n  if( !policy) {\r\n    return [];\r\n   }\r\n  if( !attenuatorsCache.has(policy)) {\r\n    const attenuators=  [];\r\n    if( policy.defaultAttenuator) {\r\n      attenuators.push(policy.defaultAttenuator);\r\n     }\r\n    collectAttenuators(attenuators, policy);\r\n    attenuatorsCache.set(policy, attenuators);\r\n   }\r\n  return attenuatorsCache.get(policy);\r\n };\r\n\r\n/**\r\n * Generates a string identifying a package for policy lookup purposes.\r\n *\r\n * @param {PackageNamingKit} namingKit\r\n * @returns {string}\r\n */$h‍_once.detectAttenuators(detectAttenuators);\r\nconst generateCanonicalName=  ({ isEntry=  false, name, path})=>   {\r\n  if( isEntry) {\r\n    throw Error('Entry module cannot be identified with a canonicalName');\r\n   }\r\n  if( name===  ATTENUATORS_COMPARTMENT) {\r\n    return ATTENUATORS_COMPARTMENT;\r\n   }\r\n  return path.join('>');\r\n };\r\n\r\n/**\r\n * Verifies if a module identified by namingKit can be a dependency of a package per packagePolicy.\r\n * packagePolicy is required, when policy is not set, skipping needs to be handled by the caller.\r\n *\r\n * @param {PackageNamingKit} namingKit\r\n * @param {*} packagePolicy\r\n * @returns {boolean}\r\n */\r\nconst        dependencyAllowedByPolicy=  (namingKit, packagePolicy)=>  {\r\n  if( namingKit.isEntry) {\r\n    // dependency on entry compartment should never be allowed\r\n    return false;\r\n   }\r\n  const canonicalName=  generateCanonicalName(namingKit);\r\n  return !!policyLookupHelper(packagePolicy, 'packages', canonicalName);\r\n };$h‍_once.dependencyAllowedByPolicy(dependencyAllowedByPolicy);\r\n\r\nconst validateDependencies=  (policy, canonicalName)=>  {\r\n  const packages=  policy.resources[canonicalName].packages;\r\n  if( !packages||  isAllowingEverything(packages)) {\r\n    return;\r\n   }\r\n\r\n  const packageNames=  keys(packages);\r\n  const attenuators=  detectAttenuators(policy);\r\n  // Join attenuators with packageNames into a Set to deduplicate and check if all are listed in policy.resources\r\n  const allSpecifiers=  new Set([...packageNames, ...attenuators]);\r\n  for( const specifier of allSpecifiers) {\r\n    if( !(specifier in policy.resources)) {\r\n      throw Error(\r\n         \`Package \${q(specifier)} is allowed for \${q(\r\n          canonicalName)\r\n          } to import but its policy is not defined. Please add a policy for \${q(\r\n          specifier)\r\n          }\`);\r\n\r\n     }\r\n   }\r\n };\r\n\r\n/**\r\n * Returns the policy applicable to the canonicalName of the package\r\n *\r\n * @param {PackageNamingKit} namingKit - a key in the policy resources spec is derived frm these\r\n * @param {object|undefined} policy - user supplied policy\r\n * @returns {object|undefined} packagePolicy if policy was specified\r\n */\r\nconst        getPolicyForPackage=  (namingKit, policy)=>  {\r\n  if( !policy) {\r\n    return undefined;\r\n   }\r\n  if( namingKit.isEntry) {\r\n    return policy.entry;\r\n   }\r\n  const canonicalName=  generateCanonicalName(namingKit);\r\n  if( canonicalName===  ATTENUATORS_COMPARTMENT) {\r\n    return {\r\n      defaultAttenuator: policy.defaultAttenuator,\r\n      packages: detectAttenuators(policy).reduce((packages, specifier)=>  {\r\n        packages[specifier]=  true;\r\n        return packages;\r\n       },{})};\r\n\r\n   }\r\n  if( policy.resources&&  policy.resources[canonicalName]) {\r\n    validateDependencies(policy, canonicalName);\r\n    return policy.resources[canonicalName];\r\n   }else {\r\n    console.warn(\r\n       \`No policy for '\${canonicalName}', omitting from compartment map.\`);\r\n\r\n    return undefined;\r\n   }\r\n };$h‍_once.getPolicyForPackage(getPolicyForPackage);\r\n\r\nconst getGlobalsList=  (packagePolicy)=>{\r\n  if( !packagePolicy.globals) {\r\n    return [];\r\n   }\r\n  // TODO: handle 'write' policy: https://github.com/endojs/endo/issues/1482\r\n  return entries(packagePolicy.globals).\r\n     filter(([_key, value])=>  value).\r\n     map(([key, _vvalue])=>  key);\r\n };\r\n\r\nconst GLOBAL_ATTENUATOR=  'attenuateGlobals';\r\nconst MODULE_ATTENUATOR=  'attenuateModule';\r\n/**\r\n *\r\n * @param {AttenuationDefinition} attenuationDefinition\r\n * @param {DeferredAttenuatorsProvider} attenuatorsProvider\r\n * @param {string} attenuatorExportName\r\n * @returns {Promise<Function>}\r\n */\r\nconst importAttenuatorForDefinition=  async(\r\n  attenuationDefinition,\r\n  attenuatorsProvider,\r\n  attenuatorExportName)=>\r\n     {\r\n  if( !attenuatorsProvider) {\r\n    throw Error( \`attenuatorsProvider is required to import attenuators\`);\r\n   }\r\n  const { specifier, params, displayName}=   getAttenuatorFromDefinition(\r\n    attenuationDefinition);\r\n\r\n  const attenuator=  await attenuatorsProvider.import(specifier);\r\n  if( !attenuator[attenuatorExportName]) {\r\n    throw Error(\r\n       \`Attenuator \${q(displayName)} does not export \${q(attenuatorExportName)}\`);\r\n\r\n   }\r\n  // TODO: uncurry bind for security?\r\n  const attenuate=  attenuator[attenuatorExportName].bind(attenuator, params);\r\n  return attenuate;\r\n };\r\n\r\n/**\r\n *\r\n * @param {Record<string, Compartment>} compartments\r\n * @param {Record<string, CompartmentDescriptor>} compartmentDescriptors\r\n * @returns {DeferredAttenuatorsProvider}\r\n */\r\nconst        makeDeferredAttenuatorsProvider=  (\r\n  compartments,\r\n  compartmentDescriptors)=>\r\n     {\r\n  let importAttenuator;\r\n  let defaultAttenuator;\r\n  // Attenuators compartment is not created when there's no policy.\r\n  // Errors should be thrown when the provider is used.\r\n  if( !compartmentDescriptors[ATTENUATORS_COMPARTMENT]) {\r\n    importAttenuator=  async()=>   {\r\n      throw Error( \`No attenuators specified in policy\`);\r\n     };\r\n   }else {\r\n    defaultAttenuator=\r\n      compartmentDescriptors[ATTENUATORS_COMPARTMENT].policy.defaultAttenuator;\r\n\r\n    // At the time of this function being called, attenuators compartment won't\r\n    // exist yet, we need to defer looking it up in compartments to the time of\r\n    // the import function being called.\r\n    /**\r\n     *\r\n     * @param {string} attenuatorSpecifier\r\n     * @returns {Promise<Attenuator>}\r\n     */\r\n    importAttenuator=  async(attenuatorSpecifier)=> {\r\n      if( !attenuatorSpecifier) {\r\n        if( !defaultAttenuator) {\r\n          throw Error( \`No default attenuator specified in policy\`);\r\n         }\r\n        attenuatorSpecifier=  defaultAttenuator;\r\n       }\r\n      const { namespace}=   await compartments[ATTENUATORS_COMPARTMENT].import(\r\n        attenuatorSpecifier);\r\n\r\n      return namespace;\r\n     };\r\n   }\r\n\r\n  return {\r\n    import: importAttenuator};\r\n\r\n };\r\n\r\n/**\r\n *\r\n * @param {object} options\r\n * @param {DeferredAttenuatorsProvider} options.attenuators\r\n * @param {AttenuationDefinition} options.attenuationDefinition\r\n * @param {object} options.globalThis\r\n * @param {object} options.globals\r\n */$h‍_once.makeDeferredAttenuatorsProvider(makeDeferredAttenuatorsProvider);\r\nasync function attenuateGlobalThis({\r\n  attenuators,\r\n  attenuationDefinition,\r\n  globalThis,\r\n  globals})\r\n   {\r\n  const attenuate=  await importAttenuatorForDefinition(\r\n    attenuationDefinition,\r\n    attenuators,\r\n    GLOBAL_ATTENUATOR);\r\n\r\n\r\n  // attenuate can either define properties on globalThis on its own,\r\n  // or return an object with properties to transfer onto globalThis.\r\n  // The latter is consistent with how module attenuators work so that\r\n  // one attenuator implementation can be used for both if use of\r\n  // defineProperty is not needed for attenuating globals.\r\n  const result=  await attenuate(globals, globalThis);\r\n  if( typeof result===  'object'&&  result!==  null) {\r\n    assign(globalThis, result);\r\n   }\r\n }\r\n\r\n/**\r\n * Filters available globals and returns a copy according to the policy\r\n *\r\n * @param {object} globalThis\r\n * @param {object} globals\r\n * @param {object} packagePolicy\r\n * @param {DeferredAttenuatorsProvider} attenuators\r\n * @param {Array<Promise>} pendingJobs\r\n * @param {string} name\r\n * @returns {void}\r\n */\r\nconst        attenuateGlobals=  (\r\n  globalThis,\r\n  globals,\r\n  packagePolicy,\r\n  attenuators,\r\n  pendingJobs,\r\n  name=  '<unknown>')=>\r\n     {\r\n  let freezeGlobalThisUnlessOptedOut=  ()=>  {\r\n    freeze(globalThis);\r\n   };\r\n  if( packagePolicy&&  packagePolicy.noGlobalFreeze) {\r\n    freezeGlobalThisUnlessOptedOut=  ()=>  { };\r\n   }\r\n  if( !packagePolicy||  isAllowingEverything(packagePolicy.globals)) {\r\n    selectiveCopy(globals, globalThis);\r\n    freezeGlobalThisUnlessOptedOut();\r\n    return;\r\n   }\r\n  if( isAttenuationDefinition(packagePolicy.globals)) {\r\n    const attenuationDefinition=  packagePolicy.globals;\r\n    const { displayName}=   getAttenuatorFromDefinition(attenuationDefinition);\r\n    const attenuationPromise=  Promise.resolve() // delay to next tick while linking is synchronously finalized\r\n.      then(()=>\r\n        attenuateGlobalThis({\r\n          attenuators,\r\n          attenuationDefinition,\r\n          globalThis,\r\n          globals})).\r\n\r\n\r\n       then(freezeGlobalThisUnlessOptedOut, (error)=>{\r\n        freezeGlobalThisUnlessOptedOut();\r\n        throw Error(\r\n           \`Error while attenuating globals for \${q(name)} with \${q(\r\n            displayName)\r\n            }: \${q(error.message)}\` // TODO: consider an option to expose stacktrace for ease of debugging\r\n);\r\n       });\r\n    pendingJobs.push(attenuationPromise);\r\n\r\n    return;\r\n   }\r\n  const list=  getGlobalsList(packagePolicy);\r\n  selectiveCopy(globals, globalThis, list);\r\n  freezeGlobalThisUnlessOptedOut();\r\n };\r\n\r\n/**\r\n * Throws if importing of the specifier is not allowed by the policy\r\n *\r\n * @param {string} specifier\r\n * @param {object} compartmentDescriptor\r\n * @param {object} [info]\r\n */$h‍_once.attenuateGlobals(attenuateGlobals);\r\nconst        enforceModulePolicy=  (specifier, compartmentDescriptor, info)=>  {\r\n  const { policy, modules}=   compartmentDescriptor;\r\n  if( !policy) {\r\n    return;\r\n   }\r\n\r\n  if( !info.exit) {\r\n    if( !modules[specifier]) {\r\n      throw Error(\r\n         \`Importing '\${specifier}' was not allowed by policy packages:\${q(\r\n          policy.packages)\r\n          }\`);\r\n\r\n     }\r\n    return;\r\n   }\r\n\r\n  if( !policyLookupHelper(policy, 'builtins', specifier)) {\r\n    throw Error(\r\n       \`Importing '\${specifier}' was not allowed by policy 'builtins':\${q(\r\n        policy.builtins)\r\n        }\`);\r\n\r\n   }\r\n };\r\n\r\n/**\r\n *\r\n * @param {object} options\r\n * @param {DeferredAttenuatorsProvider} options.attenuators\r\n * @param {AttenuationDefinition} options.attenuationDefinition\r\n * @param {ModuleExportsNamespace} options.originalModule\r\n * @returns {ModuleExportsNamespace}\r\n */$h‍_once.enforceModulePolicy(enforceModulePolicy);\r\nfunction attenuateModule({\r\n  attenuators,\r\n  attenuationDefinition,\r\n  originalModule})\r\n   {\r\n  const attenuationCompartment=  new Compartment(\r\n    {},\r\n    {},\r\n    {\r\n      resolveHook: (moduleSpecifier)=>moduleSpecifier,\r\n      importHook: async()=>   {\r\n        const attenuate=  await importAttenuatorForDefinition(\r\n          attenuationDefinition,\r\n          attenuators,\r\n          MODULE_ATTENUATOR);\r\n\r\n        const ns=  await attenuate(originalModule);\r\n        const staticModuleRecord=  freeze({\r\n          imports: [],\r\n          exports: keys(ns),\r\n          execute: (moduleExports)=>{\r\n            assign(moduleExports, ns);\r\n           }});\r\n\r\n        return staticModuleRecord;\r\n       }});\r\n\r\n\r\n  return attenuationCompartment.module('.');\r\n }\r\n\r\n/**\r\n * Throws if importing of the specifier is not allowed by the policy\r\n *\r\n * @param {string} specifier - exit module name\r\n * @param {object} originalModule - reference to the exit module\r\n * @param {object} policy - local compartment policy\r\n * @param {DeferredAttenuatorsProvider} attenuators - a key-value where attenuations can be found\r\n */\r\nconst        attenuateModuleHook=  (\r\n  specifier,\r\n  originalModule,\r\n  policy,\r\n  attenuators)=>\r\n     {\r\n  const policyValue=  policyLookupHelper(policy, 'builtins', specifier);\r\n  if( !policy||  policyValue===  true) {\r\n    return originalModule;\r\n   }\r\n\r\n  if( !policyValue) {\r\n    throw Error(\r\n       \`Attenuation failed '\${specifier}' was not in policy builtins:\${q(\r\n        policy.builtins)\r\n        }\`);\r\n\r\n   }\r\n\r\n  return attenuateModule({\r\n    attenuators,\r\n    attenuationDefinition: policyValue,\r\n    originalModule});\r\n\r\n };$h‍_once.attenuateModuleHook(attenuateModuleHook);\r\n\r\nconst padDiagnosis=  (text)=> \` (\${text})\`;\r\n/**\r\n * Provide dignostic information for a missing compartment error\r\n *\r\n * @param {object}  args\r\n * @param {string}  args.moduleSpecifier\r\n * @param {object}  args.compartmentDescriptor\r\n * @param {string}  args.foreignModuleSpecifier\r\n * @param {string}  args.foreignCompartmentName\r\n * @returns {string}\r\n */\r\nconst        diagnoseMissingCompartmentError=  ({\r\n  moduleSpecifier,\r\n  compartmentDescriptor,\r\n  foreignModuleSpecifier,\r\n  foreignCompartmentName})=>\r\n      {\r\n  const { policy, name, scopes}=   compartmentDescriptor;\r\n\r\n  if( policy) {\r\n    if( !policy.packages) {\r\n      return padDiagnosis(\r\n         \`There were no allowed packages specified in policy for \${q(name)}\`);\r\n\r\n     }\r\n    if( name===  ATTENUATORS_COMPARTMENT) {\r\n      return padDiagnosis(\r\n         \`Attenuator \${q(\r\n          moduleSpecifier)\r\n          } was imported but there is no policy resources entry defined for it.\`);\r\n\r\n     }\r\n\r\n    const scopeNames=  entries(scopes).\r\n       filter(([_name, scope])=>  scope.compartment===  foreignCompartmentName).\r\n       map(([scopeName])=>  scopeName);\r\n    if( scopeNames.length===  1&&  scopeNames[0]===  moduleSpecifier) {\r\n      return padDiagnosis(\r\n         \`Package \${q(\r\n          moduleSpecifier)\r\n          } is missing. Are you sure there is an entry in policy resources specified for it?\`);\r\n\r\n     }else {\r\n      return padDiagnosis(\r\n         \`Package \${q(moduleSpecifier)} resolves to \${q(\r\n          foreignModuleSpecifier)\r\n          } in \${q(\r\n          foreignCompartmentName)\r\n          } which seems disallowed by policy. There is likely an override defined that causes another package to be imported as \${q(\r\n          moduleSpecifier)\r\n          }.\`);\r\n\r\n     }\r\n   }\r\n  // Omit diagnostics when parent package had no policy - it means there was no policy.\r\n  return '';\r\n };$h‍_once.diagnoseMissingCompartmentError(diagnoseMissingCompartmentError);\r\n})\r\n\r\n,// === functors[6] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let resolve,parseExtension,enforceModulePolicy,attenuateModuleHook,ATTENUATORS_COMPARTMENT,diagnoseMissingCompartmentError,attenuateGlobals,makeDeferredAttenuatorsProvider;$h‍_imports([["./node-module-specifier.js", [["resolve", [$h‍_a => (resolve = $h‍_a)]]]],["./extension.js", [["parseExtension", [$h‍_a => (parseExtension = $h‍_a)]]]],["./policy.js", [["enforceModulePolicy", [$h‍_a => (enforceModulePolicy = $h‍_a)]],["attenuateModuleHook", [$h‍_a => (attenuateModuleHook = $h‍_a)]],["ATTENUATORS_COMPARTMENT", [$h‍_a => (ATTENUATORS_COMPARTMENT = $h‍_a)]],["diagnoseMissingCompartmentError", [$h‍_a => (diagnoseMissingCompartmentError = $h‍_a)]],["attenuateGlobals", [$h‍_a => (attenuateGlobals = $h‍_a)]],["makeDeferredAttenuatorsProvider", [$h‍_a => (makeDeferredAttenuatorsProvider = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nconst { entries, fromEntries}=   Object;\r\nconst { hasOwnProperty}=   Object.prototype;\r\nconst { apply}=   Reflect;\r\nconst { allSettled}=   Promise;\r\n\r\n/**\r\n * @template T\r\n * @type {(iterable: Iterable<ERef<T>>) => Promise<Array<PromiseSettledResult<T>>>}\r\n */\r\nconst promiseAllSettled=  allSettled.bind(Promise);\r\n\r\nconst inertStaticModuleRecord=  {\r\n  imports: [],\r\n  exports: [],\r\n  execute() {\r\n    throw new Error(\r\n       \`Assertion failed: compartment graphs built for archives cannot be initialized\`);\r\n\r\n   }};\r\n\r\n\r\nconst inertModuleNamespace=  new Compartment(\r\n  {},\r\n  {},\r\n  {\r\n    resolveHook() {\r\n      return '';\r\n     },\r\n          async importHook(){\r\n      return inertStaticModuleRecord;\r\n     }}).\r\n\r\n  module('');\r\n\r\nconst defaultCompartment=  Compartment;\r\n\r\n// q, as in quote, for strings in error messages.\r\nconst q=  JSON.stringify;\r\n\r\n/**\r\n * @param {Record<string, unknown>} object\r\n * @param {string} key\r\n * @returns {boolean}\r\n */\r\nconst has=  (object, key)=>  apply(hasOwnProperty, object, [key]);\r\n\r\n/**\r\n * Decide if extension is clearly indicating a parser/language for a file\r\n *\r\n * @param {string} extension\r\n * @returns {boolean}\r\n */\r\nconst extensionImpliesLanguage=  (extension)=>extension!==  'js';\r\n\r\n/**\r\n * \`makeExtensionParser\` produces a \`parser\` that parses the content of a\r\n * module according to the corresponding module language, given the extension\r\n * of the module specifier and the configuration of the containing compartment.\r\n * We do not yet support import assertions and we do not have a mechanism\r\n * for validating the MIME type of the module content against the\r\n * language implied by the extension or file name.\r\n *\r\n * @param {Record<string, string>} languageForExtension - maps a file extension\r\n * to the corresponding language.\r\n * @param {Record<string, string>} languageForModuleSpecifier - In a rare case,\r\n * the type of a module is implied by package.json and should not be inferred\r\n * from its extension.\r\n * @param {Record<string, ParserImplementation>} parserForLanguage\r\n * @param {ModuleTransforms} moduleTransforms\r\n * @returns {ParseFn}\r\n */\r\nconst makeExtensionParser=  (\r\n  languageForExtension,\r\n  languageForModuleSpecifier,\r\n  parserForLanguage,\r\n  moduleTransforms)=>\r\n     {\r\n  return async( bytes, specifier, location, packageLocation, options)=>  {\r\n    let language;\r\n    const extension=  parseExtension(location);\r\n\r\n    if(\r\n      !extensionImpliesLanguage(extension)&&\r\n      has(languageForModuleSpecifier, specifier))\r\n      {\r\n      language=  languageForModuleSpecifier[specifier];\r\n     }else {\r\n      language=  languageForExtension[extension]||  extension;\r\n     }\r\n\r\n    if( has(moduleTransforms, language)) {\r\n      ({ bytes, parser: language}=   await moduleTransforms[language](\r\n        bytes,\r\n        specifier,\r\n        location,\r\n        packageLocation));\r\n\r\n     }\r\n\r\n    if( !has(parserForLanguage, language)) {\r\n      throw new Error(\r\n         \`Cannot parse module \${specifier} at \${location}, no parser configured for the language \${language}\`);\r\n\r\n     }\r\n    const { parse}=   parserForLanguage[language];\r\n    return parse(bytes, specifier, location, packageLocation, options);\r\n   };\r\n };\r\n\r\n/**\r\n * @param {Record<string, Language>} languageForExtension\r\n * @param {Record<string, string>} languageForModuleSpecifier - In a rare case, the type of a module\r\n * is implied by package.json and should not be inferred from its extension.\r\n * @param {Record<string, ParserImplementation>} parserForLanguage\r\n * @param {ModuleTransforms} moduleTransforms\r\n * @returns {ParseFn}\r\n */\r\nconst        mapParsers=  (\r\n  languageForExtension,\r\n  languageForModuleSpecifier,\r\n  parserForLanguage,\r\n  moduleTransforms=  {})=>\r\n     {\r\n  const languageForExtensionEntries=  [];\r\n  const problems=  [];\r\n  for( const [extension, language]of  entries(languageForExtension)) {\r\n    if( has(parserForLanguage, language)) {\r\n      languageForExtensionEntries.push([extension, language]);\r\n     }else {\r\n      problems.push( \`\${q(language)} for extension \${q(extension)}\`);\r\n     }\r\n   }\r\n  if( problems.length>  0) {\r\n    throw new Error( \`No parser available for language: \${problems.join(', ')}\`);\r\n   }\r\n  return makeExtensionParser(\r\n    fromEntries(languageForExtensionEntries),\r\n    languageForModuleSpecifier,\r\n    parserForLanguage,\r\n    moduleTransforms);\r\n\r\n };\r\n\r\n/**\r\n * For a full, absolute module specifier like "dependency",\r\n * produce the module specifier in the dependency, like ".".\r\n * For a deeper path like "@org/dep/aux" and a prefix like "@org/dep", produce\r\n * "./aux".\r\n *\r\n * @param {string} moduleSpecifier\r\n * @param {string} prefix\r\n * @returns {string=}\r\n */$h‍_once.mapParsers(mapParsers);\r\nconst trimModuleSpecifierPrefix=  (moduleSpecifier, prefix)=>  {\r\n  if( moduleSpecifier===  prefix) {\r\n    return '.';\r\n   }\r\n  if( moduleSpecifier.startsWith( \`\${prefix}/\`)){\r\n    return  \`./\${moduleSpecifier.slice(prefix.length+ 1) }\`;\r\n   }\r\n  return undefined;\r\n };\r\n\r\n/**\r\n * \`makeModuleMapHook\` generates a \`moduleMapHook\` for the \`Compartment\`\r\n * constructor, suitable for Node.js style packages where any module in the\r\n * package might be imported.\r\n * Since searching for all of these modules up front is either needlessly\r\n * costly (on a file system) or impossible (from a web service), we\r\n * let the import graph guide our search.\r\n * Any module specifier with an absolute prefix should be captured by\r\n * the \`moduleMap\` or \`moduleMapHook\`.\r\n *\r\n * @param {CompartmentDescriptor} compartmentDescriptor\r\n * @param {Record<string, Compartment>} compartments\r\n * @param {string} compartmentName\r\n * @param {Record<string, ModuleDescriptor>} moduleDescriptors\r\n * @param {Record<string, ModuleDescriptor>} scopeDescriptors\r\n * @param {Record<string, string>} exitModules\r\n * @param {DeferredAttenuatorsProvider} attenuators\r\n * @param {boolean} archiveOnly\r\n * @returns {ModuleMapHook | undefined}\r\n */\r\nconst makeModuleMapHook=  (\r\n  compartmentDescriptor,\r\n  compartments,\r\n  compartmentName,\r\n  moduleDescriptors,\r\n  scopeDescriptors,\r\n  exitModules,\r\n  attenuators,\r\n  archiveOnly)=>\r\n     {\r\n  /**\r\n   * @param {string} moduleSpecifier\r\n   * @returns {string | object | undefined}\r\n   */\r\n  const moduleMapHook=  (moduleSpecifier)=>{\r\n    compartmentDescriptor.retained=  true;\r\n\r\n    const moduleDescriptor=  moduleDescriptors[moduleSpecifier];\r\n    if( moduleDescriptor!==  undefined) {\r\n      // "foreignCompartmentName" refers to the compartment which\r\n      // may differ from the current compartment\r\n      const {\r\n        compartment: foreignCompartmentName=  compartmentName,\r\n        module: foreignModuleSpecifier,\r\n        exit}=\r\n          moduleDescriptor;\r\n      if( exit!==  undefined) {\r\n        enforceModulePolicy(moduleSpecifier, compartmentDescriptor, {\r\n          exit: true});\r\n\r\n        const module=  exitModules[exit];\r\n        if( module===  undefined) {\r\n          throw new Error(\r\n             \`Cannot import missing external module \${q(\r\n              exit)\r\n              }, may be missing from \${compartmentName} package.json\`);\r\n\r\n         }\r\n        if( archiveOnly) {\r\n          return inertModuleNamespace;\r\n         }else {\r\n          return attenuateModuleHook(\r\n            exit,\r\n            module,\r\n            compartmentDescriptor.policy,\r\n            attenuators);\r\n\r\n         }\r\n       }\r\n      if( foreignModuleSpecifier!==  undefined) {\r\n        if( !moduleSpecifier.startsWith('./')) {\r\n          // archive goes through foreignModuleSpecifier for local modules too\r\n          enforceModulePolicy(moduleSpecifier, compartmentDescriptor, {\r\n            exit: false});\r\n\r\n         }\r\n\r\n        const foreignCompartment=  compartments[foreignCompartmentName];\r\n        if( foreignCompartment===  undefined) {\r\n          throw new Error(\r\n             \`Cannot import from missing compartment \${q(\r\n              foreignCompartmentName)\r\n              }\${diagnoseMissingCompartmentError({\r\n              moduleSpecifier,\r\n              compartmentDescriptor,\r\n              foreignModuleSpecifier,\r\n              foreignCompartmentName})\r\n               }\`);\r\n\r\n         }\r\n        return foreignCompartment.module(foreignModuleSpecifier);\r\n       }\r\n     }else if( has(exitModules, moduleSpecifier)) {\r\n      enforceModulePolicy(moduleSpecifier, compartmentDescriptor, {\r\n        exit: true});\r\n\r\n\r\n      // When linking off the filesystem as with \`importLocation\`,\r\n      // there isn't a module descriptor for every module.\r\n      moduleDescriptors[moduleSpecifier]=  { exit: moduleSpecifier};\r\n      if( archiveOnly) {\r\n        return inertModuleNamespace;\r\n       }else {\r\n        return attenuateModuleHook(\r\n          moduleSpecifier,\r\n          exitModules[moduleSpecifier],\r\n          compartmentDescriptor.policy,\r\n          attenuators);\r\n\r\n       }\r\n     }\r\n\r\n    // Search for a scope that shares a prefix with the requested module\r\n    // specifier.\r\n    // This might be better with a trie, but only a benchmark on real-world\r\n    // data would tell us whether the additional complexity would translate to\r\n    // better performance, so this is left readable and presumed slow for now.\r\n    for( const [scopePrefix, scopeDescriptor]of  entries(scopeDescriptors)) {\r\n      const foreignModuleSpecifier=  trimModuleSpecifierPrefix(\r\n        moduleSpecifier,\r\n        scopePrefix);\r\n\r\n\r\n      if( foreignModuleSpecifier!==  undefined) {\r\n        const { compartment: foreignCompartmentName}=   scopeDescriptor;\r\n        if( foreignCompartmentName===  undefined) {\r\n          throw new Error(\r\n             \`Cannot import from scope \${scopePrefix} due to missing "compartment" property\`);\r\n\r\n         }\r\n        const foreignCompartment=  compartments[foreignCompartmentName];\r\n        if( foreignCompartment===  undefined) {\r\n          throw new Error(\r\n             \`Cannot import from missing compartment \${q(\r\n              foreignCompartmentName)\r\n              }\${diagnoseMissingCompartmentError({\r\n              moduleSpecifier,\r\n              compartmentDescriptor,\r\n              foreignModuleSpecifier,\r\n              foreignCompartmentName})\r\n               }\`);\r\n\r\n         }\r\n\r\n        // Despite all non-exit modules not allowed by policy being dropped\r\n        // while building the graph, this check is necessary because module\r\n        // is written back to the compartment map below.\r\n        enforceModulePolicy(scopePrefix, compartmentDescriptor, {\r\n          exit: false});\r\n\r\n        // The following line is weird.\r\n        // Information is flowing backward.\r\n        // This moduleMapHook writes back to the \`modules\` descriptor, from the\r\n        // original compartment map.\r\n        // So the compartment map that was used to create the compartment\r\n        // assembly, can then be captured in an archive, obviating the need for\r\n        // a moduleMapHook when we assemble compartments from the resulting\r\n        // archive.\r\n        moduleDescriptors[moduleSpecifier]=  {\r\n          compartment: foreignCompartmentName,\r\n          module: foreignModuleSpecifier};\r\n\r\n        return foreignCompartment.module(foreignModuleSpecifier);\r\n       }\r\n     }\r\n\r\n    // No entry in the module map.\r\n    // Compartments will fall through to their \`importHook\`.\r\n    return undefined;\r\n   };\r\n\r\n  return moduleMapHook;\r\n };\r\n\r\n/**\r\n * Assemble a DAG of compartments as declared in a compartment map starting at\r\n * the named compartment and building all compartments that it depends upon,\r\n * recursively threading the modules exported by one compartment into the\r\n * compartment that imports them.\r\n * Returns the root of the compartment DAG.\r\n * Does not load or execute any modules.\r\n * Uses makeImportHook with the given "location" string of each compartment in\r\n * the DAG.\r\n * Passes the given globals and external modules into the root compartment\r\n * only.\r\n *\r\n * @param {CompartmentMapDescriptor} compartmentMap\r\n * @param {LinkOptions} options\r\n */\r\nconst        link=  (\r\n  { entry, compartments: compartmentDescriptors},\r\n  {\r\n    makeImportHook,\r\n    parserForLanguage,\r\n    globals=  {},\r\n    transforms=  [],\r\n    moduleTransforms=  {},\r\n    __shimTransforms__=  [],\r\n    modules: exitModules=  {},\r\n    archiveOnly=  false,\r\n    Compartment=  defaultCompartment})=>\r\n\r\n     {\r\n  const { compartment: entryCompartmentName}=   entry;\r\n\r\n  /** @type {Record<string, Compartment>} */\r\n  const compartments=  Object.create(null);\r\n\r\n  /**\r\n   * @param {string} attenuatorSpecifier\r\n   */\r\n  const attenuators=  makeDeferredAttenuatorsProvider(\r\n    compartments,\r\n    compartmentDescriptors);\r\n\r\n\r\n  /** @type {Record<string, ResolveHook>} */\r\n  const resolvers=  Object.create(null);\r\n\r\n  const pendingJobs=  [];\r\n\r\n  for( const [compartmentName, compartmentDescriptor]of  entries(\r\n    compartmentDescriptors))\r\n     {\r\n    const {\r\n      location,\r\n      name,\r\n      modules=  Object.create(null),\r\n      parsers: languageForExtension=  Object.create(null),\r\n      types: languageForModuleSpecifier=  Object.create(null),\r\n      scopes=  Object.create(null)}=\r\n        compartmentDescriptor;\r\n\r\n    // Capture the default.\r\n    // The \`moduleMapHook\` writes back to the compartment map.\r\n    compartmentDescriptor.modules=  modules;\r\n\r\n    const parse=  mapParsers(\r\n      languageForExtension,\r\n      languageForModuleSpecifier,\r\n      parserForLanguage,\r\n      moduleTransforms);\r\n\r\n    /** @type {ShouldDeferError} */\r\n    const shouldDeferError=  (language)=>{\r\n      if( language&&  has(parserForLanguage, language)) {\r\n        return parserForLanguage[language].heuristicImports;\r\n       }else {\r\n        // If language is undefined or there's no parser, the error we could consider deferring is surely related to\r\n        // that. Nothing to throw here.\r\n        return false;\r\n       }\r\n     };\r\n\r\n    const importHook=  makeImportHook(\r\n      location,\r\n      name,\r\n      parse,\r\n      shouldDeferError,\r\n      compartments);\r\n\r\n    const moduleMapHook=  makeModuleMapHook(\r\n      compartmentDescriptor,\r\n      compartments,\r\n      compartmentName,\r\n      modules,\r\n      scopes,\r\n      exitModules,\r\n      attenuators,\r\n      archiveOnly);\r\n\r\n    const resolveHook=  resolve;\r\n    resolvers[compartmentName]=  resolve;\r\n\r\n    const compartment=  new Compartment(Object.create(null), undefined, {\r\n      resolveHook,\r\n      importHook,\r\n      moduleMapHook,\r\n      transforms,\r\n      __shimTransforms__,\r\n      name: location});\r\n\r\n\r\n    if( !archiveOnly) {\r\n      attenuateGlobals(\r\n        compartment.globalThis,\r\n        globals,\r\n        compartmentDescriptor.policy,\r\n        attenuators,\r\n        pendingJobs,\r\n        compartmentDescriptor.name);\r\n\r\n     }\r\n\r\n    compartments[compartmentName]=  compartment;\r\n   }\r\n\r\n  const compartment=  compartments[entryCompartmentName];\r\n  if( compartment===  undefined) {\r\n    throw new Error(\r\n       \`Cannot assemble compartment graph because the root compartment named \${q(\r\n        entryCompartmentName)\r\n        } is missing from the compartment map\`);\r\n\r\n   }\r\n  const attenuatorsCompartment=  compartments[ATTENUATORS_COMPARTMENT];\r\n\r\n  return {\r\n    compartment,\r\n    compartments,\r\n    resolvers,\r\n    attenuatorsCompartment,\r\n    pendingJobsPromise: promiseAllSettled(pendingJobs).then(\r\n      /** @param {PromiseSettledResult<unknown>[]} results */ (results)=>{\r\n        const errors=  results.\r\n           filter((result)=>result.status===  'rejected').\r\n           map(\r\n            /** @param {PromiseRejectedResult} result */ (result)=>\r\n              result.reason);\r\n\r\n        if( errors.length>  0) {\r\n          throw new Error(\r\n             \`Globals attenuation errors: \${errors.\r\n               map((error)=>error.message).\r\n               join(', ') }\`);\r\n\r\n         }\r\n       })};\r\n\r\n\r\n };\r\n\r\n/**\r\n * @param {CompartmentMapDescriptor} compartmentMap\r\n * @param {LinkOptions} options\r\n */$h‍_once.link(link);\r\nconst        assemble=  (compartmentMap, options)=>\r\n  link(compartmentMap, options).compartment;$h‍_once.assemble(assemble);\r\n})\r\n\r\n,// === functors[7] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n/**\r\n * TypeScript cannot be relied upon to deal with the nuances of Readonly, so we\r\n * borrow the pass-through type definition of harden here.\r\n *\r\n * @type {__import__('ses').Harden}\r\n */\r\nconst freeze=  Object.freeze;\r\n\r\n/** @type {__import__('./types.js').ParseFn} */\r\nconst        parseBytes=  async(\r\n  bytes,\r\n  _specifier,\r\n  _location,\r\n  _packageLocation)=>\r\n     {\r\n  // Snapshot ArrayBuffer\r\n  const buffer=  new ArrayBuffer(bytes.length);\r\n  const bytesView=  new Uint8Array(buffer);\r\n  bytesView.set(bytes);\r\n\r\n  /** @type {Array<string>} */\r\n  const imports=  freeze([]);\r\n\r\n  /**\r\n   * @param {object} exports\r\n   */\r\n  const execute=  (exports)=>{\r\n    exports.default=  buffer;\r\n   };\r\n\r\n  return {\r\n    parser: 'bytes',\r\n    bytes,\r\n    record: freeze({\r\n      imports,\r\n      exports: freeze(['default']),\r\n      execute: freeze(execute)})};\r\n\r\n\r\n };\r\n\r\n/** @type {__import__('./types.js').ParserImplementation} */$h‍_once.parseBytes(parseBytes);\r\nconst{default:$c‍_default}={default:{\r\n  parse: parseBytes,\r\n  heuristicImports: false}};$h‍_once.default($c‍_default);\r\n})\r\n\r\n,// === functors[8] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let parseLocatedJson;$h‍_imports([["./json.js", [["parseLocatedJson", [$h‍_a => (parseLocatedJson = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n/**\r\n * TypeScript cannot be relied upon to deal with the nuances of Readonly, so we\r\n * borrow the pass-through type definition of harden here.\r\n *\r\n * @type {__import__('ses').Harden}\r\n */\r\nconst freeze=  Object.freeze;\r\n\r\nconst textDecoder=  new TextDecoder();\r\n\r\n/** @type {__import__('./types.js').ParseFn} */\r\nconst        parseJson=  async(\r\n  bytes,\r\n  _specifier,\r\n  location,\r\n  _packageLocation)=>\r\n     {\r\n  const source=  textDecoder.decode(bytes);\r\n  const imports=  freeze([]);\r\n\r\n  /**\r\n   * @param {object} exports\r\n   */\r\n  const execute=  (exports)=>{\r\n    exports.default=  parseLocatedJson(source, location);\r\n   };\r\n  return {\r\n    parser: 'json',\r\n    bytes,\r\n    record: freeze({\r\n      imports,\r\n      exports: freeze(['default']),\r\n      execute: freeze(execute)})};\r\n\r\n\r\n };\r\n\r\n/** @type {__import__('./types.js').ParserImplementation} */$h‍_once.parseJson(parseJson);\r\nconst{default:$c‍_default}={default:{\r\n  parse: parseJson,\r\n  heuristicImports: false}};$h‍_once.default($c‍_default);\r\n})\r\n\r\n,// === functors[9] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n/** @typedef {__import__('./types.js').ReadFn} ReadFn */\r\n/** @typedef {__import__('./types.js').ReadPowers} ReadPowers */\r\n\r\nconst { apply}=   Reflect;\r\nconst { freeze, keys, create, hasOwnProperty, defineProperty}=   Object;\r\n\r\n/**\r\n * @param {object} object\r\n * @param {string} key\r\n * @returns {boolean}\r\n */\r\nconst has=  (object, key)=>  apply(hasOwnProperty, object, [key]);\r\n\r\nconst noTrailingSlash=  (path)=>{\r\n  const l=  path.length-  1;\r\n  return path[l]===  '\\\\'||  path[l]===  '/'?  path.slice(0, -1):  path;\r\n };\r\n\r\n/**\r\n * Generates values for __filename and __dirname from location\r\n *\r\n * @param {ReadPowers | ReadFn | undefined} readPowers\r\n * @param {string} location\r\n * @returns {{\r\n *   filename:string|null,\r\n *   dirname: string|null\r\n * }}\r\n */\r\nconst        getModulePaths=  (readPowers, location)=>  {\r\n  if(\r\n    readPowers&&\r\n    typeof readPowers!==  'function'&&\r\n    readPowers.fileURLToPath)\r\n    {\r\n    let filename=  location;\r\n    let dirname;\r\n    try {\r\n      dirname=  new URL('./', filename).href;\r\n     }catch( _) {\r\n      return {\r\n        filename: null,\r\n        dirname: null};\r\n\r\n     }\r\n\r\n    filename=  readPowers.fileURLToPath(filename).toString();\r\n    dirname=  noTrailingSlash(readPowers.fileURLToPath(dirname).toString());\r\n\r\n    return {\r\n      filename,\r\n      dirname};\r\n\r\n   }else {\r\n    return {\r\n      filename: null,\r\n      dirname: null};\r\n\r\n   }\r\n };\r\n\r\n/**\r\n * ModuleEnvironmentRecord wrapper\r\n * Creates shared export processing primitives to be used both Location and Archive usecases of cjs\r\n *\r\n * @param {object} in\r\n * @param {object} in.moduleEnvironmentRecord\r\n * @param {Compartment} in.compartment\r\n * @param {Record<string, string>} in.resolvedImports\r\n * @param {string} in.location\r\n * @param {ReadFn|ReadPowers} in.readPowers\r\n * @returns {{\r\n *   module: { exports: any },\r\n *   moduleExports: any,\r\n *   afterExecute: Function,\r\n *   require: Function,\r\n * }}\r\n */$h‍_once.getModulePaths(getModulePaths);\r\nconst        wrap=  ({\r\n  moduleEnvironmentRecord,\r\n  compartment,\r\n  resolvedImports,\r\n  location,\r\n  readPowers})=>\r\n      {\r\n  // This initial default value makes things like exports.hasOwnProperty() work in cjs.\r\n  moduleEnvironmentRecord.default=  create(\r\n    compartment.globalThis.Object.prototype);\r\n\r\n\r\n  // Set all exported properties on the defult and call namedExportProp to add them on the namespace for import *.\r\n  // Root namespace is only accessible for imports. Requiring from cjs gets the default field of the namespace.\r\n  const promoteToNamedExport=  (prop, value)=>  {\r\n    //  __esModule needs to be present for typescript-compiled modules to work, can't be skipped\r\n    if( prop!==  'default') {\r\n      moduleEnvironmentRecord[prop]=  value;\r\n     }\r\n   };\r\n\r\n  const originalExports=  new Proxy(moduleEnvironmentRecord.default, {\r\n    set(_target, prop, value) {\r\n      promoteToNamedExport(prop, value);\r\n      moduleEnvironmentRecord.default[prop]=  value;\r\n      return true;\r\n     },\r\n    defineProperty(target, prop, descriptor) {\r\n      if( has(descriptor, 'value')) {\r\n        // This will result in non-enumerable properties being enumerable for named import purposes. We could check\r\n        // enumerable here, but I don't see possible benefits of such restriction.\r\n        promoteToNamedExport(prop, descriptor.value);\r\n       }\r\n      // All the defineProperty trickery with getters used for lazy initialization will work. The trap is here only to\r\n      // elevate the values with namedExportProp whenever possible. Replacing getters with wrapped ones to facilitate\r\n      // propagating the lazy value to the namespace is not possible because defining a property with modified\r\n      // descriptor.get in the trap will cause an error.\r\n      // Object.defineProperty is used instead of Reflect.defineProperty for better error messages.\r\n      defineProperty(target, prop, descriptor);\r\n      return true;\r\n     }});\r\n\r\n\r\n  let finalExports=  originalExports;\r\n\r\n  const module=  freeze({\r\n    get exports() {\r\n      return finalExports;\r\n     },\r\n    set exports(value) {\r\n      finalExports=  value;\r\n     }});\r\n\r\n\r\n  const require=  (/** @type {string} */ importSpecifier)=>  {\r\n    const namespace=  compartment.importNow(resolvedImports[importSpecifier]);\r\n    // If you read this file carefully, you'll see it's not possible for a cjs module to not have the default anymore.\r\n    // It's currently possible to require modules that were not created by this file though.\r\n    if( has(namespace, 'default')) {\r\n      return namespace.default;\r\n     }else {\r\n      return namespace;\r\n     }\r\n   };\r\n  if( typeof readPowers===  'object'&&  readPowers.requireResolve) {\r\n    const { requireResolve}=   readPowers;\r\n    require.resolve=  freeze((specifier, options)=>\r\n      requireResolve(location, specifier, options));\r\n\r\n   }else {\r\n    require.resolve=  freeze((specifier)=>{\r\n      const error=  Error(\r\n         \`Cannot find module '\${specifier}'\\nAdd requireResolve to Endo Compartment Mapper readPowers.\`);\r\n\r\n      defineProperty(error, 'code', { value: 'MODULE_NOT_FOUND'});\r\n      throw error;\r\n     });\r\n   }\r\n\r\n  freeze(require);\r\n\r\n  const afterExecute=  ()=>  {\r\n    const exportsHaveBeenOverwritten=  finalExports!==  originalExports;\r\n    // Promotes keys from redefined module.export to top level namespace for import *\r\n    // Note: We could do it less consistently but closer to how node does it if we iterated over exports detected by\r\n    // the lexer.\r\n    if( exportsHaveBeenOverwritten) {\r\n      moduleEnvironmentRecord.default=  finalExports;\r\n      keys(moduleEnvironmentRecord.default||  {}).forEach((prop)=>{\r\n        if( prop!==  'default')\r\n          moduleEnvironmentRecord[prop]=  moduleEnvironmentRecord.default[prop];\r\n       });\r\n     }\r\n   };\r\n\r\n  return {\r\n    module,\r\n    moduleExports: originalExports,\r\n    afterExecute,\r\n    require};\r\n\r\n };$h‍_once.wrap(wrap);\r\n})\r\n\r\n,// === functors[10] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let parseLocatedJson,wrap,getModulePaths;$h‍_imports([["./json.js", [["parseLocatedJson", [$h‍_a => (parseLocatedJson = $h‍_a)]]]],["./parse-cjs-shared-export-wrapper.js", [["wrap", [$h‍_a => (wrap = $h‍_a)]],["getModulePaths", [$h‍_a => (getModulePaths = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n\r\nconst textDecoder=  new TextDecoder();\r\n\r\n/** @type {__import__('./types.js').ParseFn} */\r\nconst        parsePreCjs=  async(\r\n  bytes,\r\n  _specifier,\r\n  location,\r\n  _packageLocation,\r\n  { readPowers}=   {})=>\r\n     {\r\n  const text=  textDecoder.decode(bytes);\r\n  const { source, imports, exports, reexports}=   parseLocatedJson(\r\n    text,\r\n    location);\r\n\r\n\r\n  const { filename, dirname}=   await getModulePaths(readPowers, location);\r\n\r\n  const staticModuleRecord=  {\r\n    imports,\r\n    reexports,\r\n    exports,\r\n    /**\r\n     * @param {object} moduleEnvironmentRecord\r\n     * @param {Compartment} compartment\r\n     * @param {Record<string, string>} resolvedImports\r\n     */\r\n    execute(moduleEnvironmentRecord, compartment, resolvedImports) {\r\n      let functor;\r\n      /* eslint-disable-next-line no-underscore-dangle */\r\n      const syncModuleFunctor=  staticModuleRecord.__syncModuleFunctor__;\r\n      if( syncModuleFunctor!==  undefined) {\r\n        functor=  syncModuleFunctor;\r\n       }else {\r\n        functor=  compartment.evaluate(source);\r\n       }\r\n\r\n      const { require, moduleExports, module, afterExecute}=   wrap({\r\n        moduleEnvironmentRecord,\r\n        compartment,\r\n        resolvedImports,\r\n        location,\r\n        readPowers});\r\n\r\n\r\n      functor(require, moduleExports, module, filename, dirname);\r\n\r\n      afterExecute();\r\n     }};\r\n\r\n\r\n  return {\r\n    parser: 'pre-cjs-json',\r\n    bytes,\r\n    record: staticModuleRecord};\r\n\r\n };\r\n\r\n/** @type {__import__('./types.js').ParserImplementation} */$h‍_once.parsePreCjs(parsePreCjs);\r\nconst{default:$c‍_default}={default:{\r\n  parse: parsePreCjs,\r\n  heuristicImports: true}};$h‍_once.default($c‍_default);\r\n})\r\n\r\n,// === functors[11] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let parseLocatedJson;$h‍_imports([["./json.js", [["parseLocatedJson", [$h‍_a => (parseLocatedJson = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\nconst textDecoder=  new TextDecoder();\r\n\r\n/** @type {__import__('./types.js').ParseFn} */\r\nconst        parsePreMjs=  async(\r\n  bytes,\r\n  _specifier,\r\n  location,\r\n  _packageLocation)=>\r\n     {\r\n  const text=  textDecoder.decode(bytes);\r\n  const record=  parseLocatedJson(text, location);\r\n  // eslint-disable-next-line no-underscore-dangle\r\n  record.__syncModuleProgram__+=   \`//# sourceURL=\${location}\\n\`;\r\n  return {\r\n    parser: 'pre-mjs-json',\r\n    bytes,\r\n    record};\r\n\r\n };\r\n\r\n/** @type {__import__('./types.js').ParserImplementation} */$h‍_once.parsePreMjs(parsePreMjs);\r\nconst{default:$c‍_default}={default:{\r\n  parse: parsePreMjs,\r\n  heuristicImports: false}};$h‍_once.default($c‍_default);\r\n})\r\n\r\n,// === functors[12] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n/**\r\n * TypeScript cannot be relied upon to deal with the nuances of Readonly, so we\r\n * borrow the pass-through type definition of harden here.\r\n *\r\n * @type {__import__('ses').Harden}\r\n */\r\nconst freeze=  Object.freeze;\r\n\r\nconst textDecoder=  new TextDecoder();\r\n\r\n/** @type {__import__('./types.js').ParseFn} */\r\nconst        parseText=  async(\r\n  bytes,\r\n  _specifier,\r\n  _location,\r\n  _packageLocation)=>\r\n     {\r\n  const text=  textDecoder.decode(bytes);\r\n\r\n  /** @type {Array<string>} */\r\n  const imports=  freeze([]);\r\n\r\n  /**\r\n   * @param {object} exports\r\n   */\r\n  const execute=  (exports)=>{\r\n    exports.default=  text;\r\n   };\r\n\r\n  return {\r\n    parser: 'text',\r\n    bytes,\r\n    record: freeze({\r\n      imports,\r\n      exports: freeze(['default']),\r\n      execute: freeze(execute)})};\r\n\r\n\r\n };\r\n\r\n/** @type {__import__('./types.js').ParserImplementation} */$h‍_once.parseText(parseText);\r\nconst{default:$c‍_default}={default:{\r\n  parse: parseText,\r\n  heuristicImports: false}};$h‍_once.default($c‍_default);\r\n})\r\n\r\n,// === functors[13] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n/** @typedef {__import__('./types.js').ReadFn} ReadFn */\r\n/** @typedef {__import__('./types.js').CanonicalFn} CanonicalFn */\r\n/** @typedef {__import__('./types.js').ReadPowers} ReadPowers */\r\n\r\n/** @type {CanonicalFn} */\r\nconst canonicalShim=  async(path)=> path;\r\n\r\n/**\r\n * @param {ReadFn | ReadPowers} powers\r\n * @returns {ReadPowers}\r\n */\r\nconst        unpackReadPowers=  (powers)=>{\r\n  if( typeof powers===  'function') {\r\n    return {\r\n      read: powers,\r\n      canonical: canonicalShim};\r\n\r\n   }\r\n  return powers;\r\n };$h‍_once.unpackReadPowers(unpackReadPowers);\r\n})\r\n\r\n,// === functors[14] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n/* eslint no-bitwise: ["off"] */\r\n\r\nconst q=  JSON.stringify;\r\n\r\n/**\r\n * @typedef {object} BufferReaderState\r\n * @property {Uint8Array} bytes\r\n * @property {DataView} data\r\n * @property {number} length\r\n * @property {number} index\r\n * @property {number} offset\r\n */\r\n\r\n/** @type {WeakMap<BufferReader, BufferReaderState>} */\r\nconst privateFields=  new WeakMap();\r\n\r\n/** @type {(bufferReader: BufferReader) => BufferReaderState} */\r\nconst privateFieldsGet=  privateFields.get.bind(privateFields);\r\n\r\nclass        BufferReader {\r\n  /**\r\n   * @param {ArrayBuffer} buffer\r\n   */\r\n             constructor(buffer){\r\n    const bytes=  new Uint8Array(buffer);\r\n    const data=  new DataView(bytes.buffer);\r\n    privateFields.set(this, {\r\n      bytes,\r\n      data,\r\n      length: bytes.length,\r\n      index: 0,\r\n      offset: 0});\r\n\r\n   }\r\n\r\n  /**\r\n   * @returns {number}\r\n   */\r\n            get length(){\r\n    return privateFieldsGet(this).length;\r\n   }\r\n\r\n  /**\r\n   * @returns {number}\r\n   */\r\n           get index(){\r\n    return privateFieldsGet(this).index;\r\n   }\r\n\r\n  /**\r\n   * @param {number} index\r\n   */\r\n           set index(index){\r\n    this.seek(index);\r\n   }\r\n\r\n  /**\r\n   * @param {number} offset\r\n   */\r\n            set offset(offset){\r\n    const fields=  privateFieldsGet(this);\r\n    if( offset>  fields.data.byteLength) {\r\n      throw new Error('Cannot set offset beyond length of underlying data');\r\n     }\r\n    if( offset<  0) {\r\n      throw new Error('Cannot set negative offset');\r\n     }\r\n    fields.offset=  offset;\r\n    fields.length=  fields.data.byteLength-  fields.offset;\r\n   }\r\n\r\n  /**\r\n   * @param {number} index\r\n   * @returns {boolean} whether the read head can move to the given absolute\r\n   * index.\r\n   */\r\n         canSeek(index){\r\n    const fields=  privateFieldsGet(this);\r\n    return index>=  0&&  fields.offset+  index<=  fields.length;\r\n   }\r\n\r\n  /**\r\n   * @param {number} index the index to check.\r\n   * @throws {Error} an Error if the index is out of bounds.\r\n   */\r\n               assertCanSeek(index){\r\n    const fields=  privateFieldsGet(this);\r\n    if( !this.canSeek(index)) {\r\n      throw new Error(\r\n         \`End of data reached (data length = \${fields.length}, asked index \${index}\`);\r\n\r\n     }\r\n   }\r\n\r\n  /**\r\n   * @param {number} index\r\n   * @returns {number} prior index\r\n   */\r\n      seek(index){\r\n    const fields=  privateFieldsGet(this);\r\n    const restore=  fields.index;\r\n    this.assertCanSeek(index);\r\n    fields.index=  index;\r\n    return restore;\r\n   }\r\n\r\n  /**\r\n   * @param {number} size\r\n   * @returns {Uint8Array}\r\n   */\r\n      peek(size){\r\n    const fields=  privateFieldsGet(this);\r\n    // Clamp size.\r\n    size=  Math.max(0, Math.min(fields.length-  fields.index, size));\r\n    if( size===  0) {\r\n      // in IE10, when using subarray(idx, idx), we get the array [0x00] instead of [].\r\n      return new Uint8Array(0);\r\n     }\r\n    const result=  fields.bytes.subarray(\r\n      fields.offset+  fields.index,\r\n      fields.offset+  fields.index+  size);\r\n\r\n    return result;\r\n   }\r\n\r\n  /**\r\n   * @param {number} offset\r\n   */\r\n         canRead(offset){\r\n    const fields=  privateFieldsGet(this);\r\n    return this.canSeek(fields.index+  offset);\r\n   }\r\n\r\n  /**\r\n   * Check that the offset will not go too far.\r\n   *\r\n   * @param {number} offset the additional offset to check.\r\n   * @throws {Error} an Error if the offset is out of bounds.\r\n   */\r\n               assertCanRead(offset){\r\n    const fields=  privateFieldsGet(this);\r\n    this.assertCanSeek(fields.index+  offset);\r\n   }\r\n\r\n  /**\r\n   * Get raw data without conversion, <size> bytes.\r\n   *\r\n   * @param {number} size the number of bytes to read.\r\n   * @returns {Uint8Array} the raw data.\r\n   */\r\n      read(size){\r\n    const fields=  privateFieldsGet(this);\r\n    this.assertCanRead(size);\r\n    const result=  this.peek(size);\r\n    fields.index+=  size;\r\n    return result;\r\n   }\r\n\r\n  /**\r\n   * @returns {number}\r\n   */\r\n           readUint8(){\r\n    const fields=  privateFieldsGet(this);\r\n    this.assertCanRead(1);\r\n    const index=  fields.offset+  fields.index;\r\n    const value=  fields.data.getUint8(index);\r\n    fields.index+=  1;\r\n    return value;\r\n   }\r\n\r\n  /**\r\n   * @returns {number}\r\n   * @param {boolean=} littleEndian\r\n   */\r\n            readUint16(littleEndian){\r\n    const fields=  privateFieldsGet(this);\r\n    this.assertCanRead(2);\r\n    const index=  fields.offset+  fields.index;\r\n    const value=  fields.data.getUint16(index, littleEndian);\r\n    fields.index+=  2;\r\n    return value;\r\n   }\r\n\r\n  /**\r\n   * @returns {number}\r\n   * @param {boolean=} littleEndian\r\n   */\r\n            readUint32(littleEndian){\r\n    const fields=  privateFieldsGet(this);\r\n    this.assertCanRead(4);\r\n    const index=  fields.offset+  fields.index;\r\n    const value=  fields.data.getUint32(index, littleEndian);\r\n    fields.index+=  4;\r\n    return value;\r\n   }\r\n\r\n  /**\r\n   * @param {number} index\r\n   * @returns {number}\r\n   */\r\n        byteAt(index){\r\n    const fields=  privateFieldsGet(this);\r\n    return fields.bytes[fields.offset+  index];\r\n   }\r\n\r\n  /**\r\n   * @param {number} offset\r\n   */\r\n      skip(offset){\r\n    const fields=  privateFieldsGet(this);\r\n    this.seek(fields.index+  offset);\r\n   }\r\n\r\n  /**\r\n   * @param {Uint8Array} expected\r\n   * @returns {boolean}\r\n   */\r\n        expect(expected){\r\n    const fields=  privateFieldsGet(this);\r\n    if( !this.matchAt(fields.index, expected)) {\r\n      return false;\r\n     }\r\n    fields.index+=  expected.length;\r\n    return true;\r\n   }\r\n\r\n  /**\r\n   * @param {number} index\r\n   * @param {Uint8Array} expected\r\n   * @returns {boolean}\r\n   */\r\n         matchAt(index,expected){\r\n    const fields=  privateFieldsGet(this);\r\n    if( index+  expected.length>  fields.length||  index<  0) {\r\n      return false;\r\n     }\r\n    for( let i=  0; i<  expected.length; i+=  1) {\r\n      if( expected[i]!==  this.byteAt(index+  i)) {\r\n        return false;\r\n       }\r\n     }\r\n    return true;\r\n   }\r\n\r\n  /**\r\n   * @param {Uint8Array} expected\r\n   */\r\n        assert(expected){\r\n    const fields=  privateFieldsGet(this);\r\n    if( !this.expect(expected)) {\r\n      throw new Error(\r\n         \`Expected \${q(expected)} at \${fields.index}, got \${this.peek(\r\n          expected.length)\r\n          }\`);\r\n\r\n     }\r\n   }\r\n\r\n  /**\r\n   * @param {Uint8Array} expected\r\n   * @returns {number}\r\n   */\r\n          findLast(expected){\r\n    const fields=  privateFieldsGet(this);\r\n    let index=  fields.length-  expected.length;\r\n    while( index>=  0&&  !this.matchAt(index, expected)) {\r\n      index-=  1;\r\n     }\r\n    return index;\r\n   }}$h‍_once.BufferReader(BufferReader);\r\n})\r\n\r\n,// === functors[15] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n\r\n// STORE is the magic number for "not compressed".\r\nconst        STORE=  0;$h‍_once.STORE(STORE);\r\n})\r\n\r\n,// === functors[16] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);Object.defineProperty(crc32, 'name', {value: "crc32"});$h‍_once.crc32(crc32);   // @ts-check\r\n/* eslint no-bitwise: ["off"] */\r\n\r\n/**\r\n * The following functions \`makeTable\` and \`crc32\` come from \`pako\`, from\r\n * pako/lib/zlib/crc32.js released under the MIT license, see pako\r\n * https://github.com/nodeca/pako/\r\n */\r\n\r\n// Use ordinary array, since untyped makes no boost here\r\n/**\r\n * @returns {Array<number>}\r\n */\r\nfunction makeTable() {\r\n  let c;\r\n  const table=  [];\r\n\r\n  for( let n=  0; n<  256; n+=  1) {\r\n    c=  n;\r\n    for( let k=  0; k<  8; k+=  1) {\r\n      c=  c&  1?  0xedb88320^   c>>>  1:   c>>>  1;\r\n     }\r\n    table[n]=  c;\r\n   }\r\n\r\n  return table;\r\n }\r\n\r\n// Initialize a table of 256 signed 32 bit integers.\r\nconst table=  makeTable();\r\n\r\n/**\r\n * @param {Uint8Array} bytes\r\n * @param {number} length\r\n * @param {number} index\r\n * @param {number} crc\r\n */\r\nfunction        crc32(bytes, length=  bytes.length, index=  0, crc=  0) {\r\n  const end=  index+  length;\r\n\r\n  crc^=  -1;\r\n\r\n  for( let i=  index; i<  end; i+=  1) {\r\n    crc=   crc>>>  8^   table[(crc^  bytes[i])&  0xff];\r\n   }\r\n\r\n  return (crc^  -1)>>>  0;\r\n }\r\n})\r\n\r\n,// === functors[17] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n/* eslint no-bitwise: ["off"] */\r\n\r\n/**\r\n * @param {string} string\r\n * @returns {Uint8Array}\r\n */\r\nfunction u(string) {\r\n  const array=  new Uint8Array(string.length);\r\n  for( let i=  0; i<  string.length; i+=  1) {\r\n    array[i]=  string.charCodeAt(i)&  0xff;\r\n   }\r\n  return array;\r\n }\r\n\r\nconst        LOCAL_FILE_HEADER=  u('PK\\x03\\x04');$h‍_once.LOCAL_FILE_HEADER(LOCAL_FILE_HEADER);\r\nconst        CENTRAL_FILE_HEADER=  u('PK\\x01\\x02');$h‍_once.CENTRAL_FILE_HEADER(CENTRAL_FILE_HEADER);\r\nconst        CENTRAL_DIRECTORY_END=  u('PK\\x05\\x06');$h‍_once.CENTRAL_DIRECTORY_END(CENTRAL_DIRECTORY_END);\r\nconst        ZIP64_CENTRAL_DIRECTORY_LOCATOR=  u('PK\\x06\\x07');$h‍_once.ZIP64_CENTRAL_DIRECTORY_LOCATOR(ZIP64_CENTRAL_DIRECTORY_LOCATOR);\r\nconst        ZIP64_CENTRAL_DIRECTORY_END=  u('PK\\x06\\x06');$h‍_once.ZIP64_CENTRAL_DIRECTORY_END(ZIP64_CENTRAL_DIRECTORY_END);\r\nconst        DATA_DESCRIPTOR=  u('PK\\x07\\x08');$h‍_once.DATA_DESCRIPTOR(DATA_DESCRIPTOR);\r\n})\r\n\r\n,// === functors[18] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   \r\n})\r\n\r\n,// === functors[19] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let crc32,signature,compression;$h‍_imports([["./types.js", []],["./crc32.js", [["crc32", [$h‍_a => (crc32 = $h‍_a)]]]],["./signature.js", [["*", [$h‍_a => (signature = $h‍_a)]]]],["./compression.js", [["*", [$h‍_a => (compression = $h‍_a)]]]]]);Object.defineProperty(readZip, 'name', {value: "readZip"});$h‍_once.readZip(readZip);   \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n// q, as in quote, for quoting strings in errors\r\nconst q=  JSON.stringify;\r\n\r\nconst MAX_VALUE_16BITS=  65535;\r\nconst MAX_VALUE_32BITS=  4294967295;\r\n\r\nconst textDecoder=  new TextDecoder();\r\n\r\n/**\r\n * @param {number} bitFlag\r\n * @returns {boolean}\r\n */\r\nfunction isEncrypted(bitFlag) {\r\n  return (bitFlag&  0x0001)===  0x0001;\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @returns {Date}\r\n * @see http://www.delorie.com/djgpp/doc/rbinter/it/65/16.html\r\n * @see http://www.delorie.com/djgpp/doc/rbinter/it/66/16.html\r\n */\r\nfunction readDosDateTime(reader) {\r\n  const dosTime=  reader.readUint32(true);\r\n  return new Date(\r\n    Date.UTC(\r\n      ( dosTime>>  25&   0x7f)+  1980, // year\r\n      ( dosTime>>  21&   0x0f)-  1, // month\r\n       dosTime>>  16&   0x1f, // day\r\n       dosTime>>  11&   0x1f, // hour\r\n       dosTime>>  5&   0x3f, // minute\r\n      (dosTime&  0x1f)<<  1  // second\r\n));\r\n\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @returns {ArchiveHeaders}\r\n */\r\nfunction readHeaders(reader) {\r\n  return {\r\n    versionNeeded: reader.readUint16(true),\r\n    bitFlag: reader.readUint16(true),\r\n    compressionMethod: reader.readUint16(true),\r\n    date: readDosDateTime(reader),\r\n    crc32: reader.readUint32(true),\r\n    compressedLength: reader.readUint32(true),\r\n    uncompressedLength: reader.readUint32(true)};\r\n\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @returns {CentralFileRecord}\r\n */\r\nfunction readCentralFileHeader(reader) {\r\n  const version=  reader.readUint8();\r\n  const madeBy=  reader.readUint8();\r\n  const headers=  readHeaders(reader);\r\n  const nameLength=  reader.readUint16(true);\r\n  const extraFieldsLength=  reader.readUint16(true);\r\n  const commentLength=  reader.readUint16(true);\r\n  const diskNumberStart=  reader.readUint16(true);\r\n  const internalFileAttributes=  reader.readUint16(true);\r\n  const externalFileAttributes=  reader.readUint32(true);\r\n  const fileStart=  reader.readUint32(true);\r\n\r\n  const name=  reader.read(nameLength);\r\n  // TODO read extra fields, particularly Zip64\r\n  reader.skip(extraFieldsLength);\r\n\r\n  if( headers.uncompressedLength===  MAX_VALUE_32BITS) {\r\n    throw new Error('Cannot read Zip64');\r\n   }\r\n  if( headers.compressedLength===  MAX_VALUE_32BITS) {\r\n    throw new Error('Cannot read Zip64');\r\n   }\r\n  if( fileStart===  MAX_VALUE_32BITS) {\r\n    throw new Error('Cannot read Zip64');\r\n   }\r\n  if( diskNumberStart===  MAX_VALUE_32BITS) {\r\n    throw new Error('Cannot read Zip64');\r\n   }\r\n\r\n  const comment=  reader.read(commentLength);\r\n\r\n  return {\r\n    name,\r\n    version,\r\n    madeBy,\r\n    ...headers,\r\n    diskNumberStart,\r\n    internalFileAttributes,\r\n    externalFileAttributes,\r\n    fileStart,\r\n    comment};\r\n\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @param {CentralDirectoryLocator} locator\r\n * @returns {Array<CentralFileRecord>}\r\n */\r\nfunction readCentralDirectory(reader, locator) {\r\n  const { centralDirectoryOffset, centralDirectoryRecords}=   locator;\r\n  reader.seek(centralDirectoryOffset);\r\n\r\n  const entries=  [];\r\n  while( reader.expect(signature.CENTRAL_FILE_HEADER)) {\r\n    const entry=  readCentralFileHeader(reader);\r\n    entries.push(entry);\r\n   }\r\n\r\n  if( centralDirectoryRecords!==  entries.length) {\r\n    // We expected some records but couldn't find ANY.\r\n    // This is really suspicious, as if something went wrong.\r\n    throw new Error(\r\n       \`Corrupted zip or bug: expected \${centralDirectoryRecords} records in central dir, got \${entries.length}\`);\r\n\r\n   }\r\n\r\n  return entries;\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @returns {LocalFileRecord}\r\n */\r\nfunction readFile(reader) {\r\n  reader.expect(signature.LOCAL_FILE_HEADER);\r\n  const headers=  readHeaders(reader);\r\n  const nameLength=  reader.readUint16(true);\r\n  const extraFieldsLength=  reader.readUint16(true);\r\n  const name=  reader.read(nameLength);\r\n  reader.skip(extraFieldsLength);\r\n  const content=  reader.read(headers.compressedLength);\r\n  return { name, ...headers, content};\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @param {Array<CentralFileRecord>} records\r\n * @returns {Array<LocalFileRecord>}\r\n */\r\nfunction readLocalFiles(reader, records) {\r\n  const files=  [];\r\n  for( const record of records) {\r\n    reader.seek(record.fileStart);\r\n    const file=  readFile(reader);\r\n    files.push(file);\r\n   }\r\n  return files;\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @returns {CentralDirectoryLocator}\r\n */\r\nfunction readBlockEndOfCentral(reader) {\r\n  if( !reader.expect(signature.CENTRAL_DIRECTORY_END)) {\r\n    throw new Error(\r\n      'Corrupt zip file, or zip file containing an unsupported variable-width end-of-archive comment, or an unsupported zip file with 64 bit sizes');\r\n\r\n   }\r\n  const diskNumber=  reader.readUint16(true);\r\n  const diskWithCentralDirStart=  reader.readUint16(true);\r\n  const centralDirectoryRecordsOnThisDisk=  reader.readUint16(true);\r\n  const centralDirectoryRecords=  reader.readUint16(true);\r\n  const centralDirectorySize=  reader.readUint32(true);\r\n  const centralDirectoryOffset=  reader.readUint32(true);\r\n  const commentLength=  reader.readUint16(true);\r\n  // Warning: the encoding depends of the system locale.\r\n  // On a Linux machine with LANG=en_US.utf8, this field is utf8 encoded.\r\n  // On a Windows machine, this field is encoded with the localized Windows\r\n  // code page.\r\n  const comment=  textDecoder.decode(reader.read(commentLength));\r\n  return {\r\n    diskNumber,\r\n    diskWithCentralDirStart,\r\n    centralDirectoryRecordsOnThisDisk,\r\n    centralDirectoryRecords,\r\n    centralDirectorySize,\r\n    centralDirectoryOffset,\r\n    comment};\r\n\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @returns {CentralDirectoryLocator}\r\n */\r\nfunction readEndOfCentralDirectoryRecord(reader) {\r\n  // Zip files are permitted to have a variable-width comment at the end of the\r\n  // "end of central directory record" and may have subsequent Zip64 headers.\r\n  // The prescribed method of finding the beginning of the "end of central\r\n  // directory record" is to seek the magic number:\r\n  //\r\n  //   reader.findLast(signature.CENTRAL_DIRECTORY_END);\r\n  //\r\n  // This introduces a number of undesirable, attackable ambiguities\r\n  // Agoric is not comfortable supporting, so we forbid the comment\r\n  // and 64 bit zip support so we can seek a predictable length\r\n  // from the end.\r\n  const centralDirectoryEnd=  reader.length-  22;\r\n  if( centralDirectoryEnd<  0) {\r\n    throw new Error('Corrupted zip: not enough content');\r\n   }\r\n  reader.seek(centralDirectoryEnd);\r\n  const locator=  readBlockEndOfCentral(reader);\r\n\r\n  // Excerpt from the zip spec:\r\n  //   4)  If one of the fields in the end of central directory\r\n  //       record is too small to hold required data, the field\r\n  //       should be set to -1 (0xFFFF or 0xFFFFFFFF) and the\r\n  //       ZIP64 format record should be created.\r\n  //   5)  The end of central directory record and the\r\n  //       Zip64 end of central directory locator record must\r\n  //       reside on the same disk when splitting or spanning\r\n  //       an archive.\r\n  const zip64=\r\n    locator.diskNumber===  MAX_VALUE_16BITS||\r\n    locator.diskWithCentralDirStart===  MAX_VALUE_16BITS||\r\n    locator.centralDirectoryRecordsOnThisDisk===  MAX_VALUE_16BITS||\r\n    locator.centralDirectoryRecords===  MAX_VALUE_16BITS||\r\n    locator.centralDirectorySize===  MAX_VALUE_32BITS||\r\n    locator.centralDirectoryOffset===  MAX_VALUE_32BITS;\r\n\r\n  if( zip64) {\r\n    throw new Error('Cannot read Zip64');\r\n   }\r\n\r\n  const {\r\n    centralDirectoryOffset,\r\n    centralDirectorySize\r\n    // zip64EndOfCentralSize\r\n}=    locator;\r\n\r\n  const expectedCentralDirectoryEnd=\r\n    centralDirectoryOffset+  centralDirectorySize;\r\n  const extraBytes=  centralDirectoryEnd-  expectedCentralDirectoryEnd;\r\n\r\n  reader.offset=  extraBytes;\r\n\r\n  return locator;\r\n }\r\n\r\n/**\r\n * @param {CentralFileRecord} centralRecord\r\n * @param {LocalFileRecord} localRecord\r\n * @param {string} archiveName\r\n */\r\nfunction checkRecords(centralRecord, localRecord, archiveName) {\r\n  const centralName=  textDecoder.decode(centralRecord.name);\r\n  const localName=  textDecoder.decode(localRecord.name);\r\n\r\n  // In some zip files created on Windows, the filename stored in the central\r\n  // dir contains "\\" instead of "/".  Strangely, the file name in the local\r\n  // directory uses "/" as specified:\r\n  // http://www.info-zip.org/FAQ.html#backslashes or APPNOTE#4.4.17.1, "All\r\n  // slashes MUST be forward slashes '/'") but there are a lot of bad zip\r\n  // generators...  Search "unzip mismatching "local" filename continuing with\r\n  // "central" filename version".\r\n  //\r\n  // The reasoning appears to be that the central directory is for\r\n  // user display and may differ, though this opens the possibility\r\n  // for spoofing attacks.\r\n  // http://seclists.org/fulldisclosure/2009/Sep/394\r\n  //\r\n  // We strike a compromise: the central directory name may vary from the local\r\n  // name exactly and only by different slashes.\r\n  if( centralName.replace(/\\\\/g, '/')!==  localName) {\r\n    throw new Error(\r\n       \`Zip integrity error: central record file name \${q(\r\n        centralName)\r\n        } must match local file name \${q(localName)} in archive \${q(\r\n        archiveName)\r\n        }\`);\r\n\r\n   }\r\n\r\n  /**\r\n   * @param {boolean} value\r\n   * @param {string} message\r\n   */\r\n  function check(value, message) {\r\n    if( !value) {\r\n      throw new Error(\r\n         \`Zip integrity error: \${message} for file \${q(\r\n          localName)\r\n          } in archive \${q(archiveName)}\`);\r\n\r\n     }\r\n   }\r\n\r\n  check(\r\n    centralRecord.bitFlag===  localRecord.bitFlag,\r\n     \`Central record bit flag \${centralRecord.bitFlag.toString(\r\n      16)\r\n      } must match local record bit flag \${localRecord.bitFlag.toString(16)}\`);\r\n\r\n  check(\r\n    centralRecord.compressionMethod===  localRecord.compressionMethod,\r\n     \`Central record compression method \${q(\r\n      centralRecord.compressionMethod)\r\n      } must match local compression method \${q(localRecord.compressionMethod)}\`);\r\n\r\n  // TODO Date integrity check would be easier on the original bytes.\r\n  // Perhaps defer decoding the underlying bytes.\r\n  check(\r\n    centralRecord.crc32===  localRecord.crc32,\r\n     \`Central record CRC-32 checksum \${centralRecord.crc32} must match local checksum \${localRecord.crc32}\`);\r\n\r\n  check(\r\n    centralRecord.compressedLength===  localRecord.compressedLength,\r\n     \`Central record compressed size \${centralRecord.compressedLength} must match local \${localRecord.compressedLength}\`);\r\n\r\n  check(\r\n    centralRecord.uncompressedLength===  localRecord.uncompressedLength,\r\n     \`Central record uncompressed size \${centralRecord.uncompressedLength} must match local \${localRecord.uncompressedLength}\`);\r\n\r\n\r\n  const checksum=  crc32(localRecord.content);\r\n  check(\r\n    checksum===  localRecord.crc32,\r\n     \`CRC-32 checksum mismatch, wanted \${localRecord.crc32} but actual content is \${checksum}\`);\r\n\r\n }\r\n\r\n/**\r\n * @param {number} externalFileAttributes\r\n */\r\nfunction modeForExternalAttributes(externalFileAttributes) {\r\n  return  externalFileAttributes>>  16&   0xffff;\r\n }\r\n\r\n/**\r\n * @param {CentralFileRecord} centralRecord\r\n * @param {LocalFileRecord} localRecord\r\n * @returns {CompressedFile}\r\n */\r\nfunction recordToFile(centralRecord, localRecord) {\r\n  const mode=  modeForExternalAttributes(centralRecord.externalFileAttributes);\r\n  return {\r\n    name: centralRecord.name,\r\n    mode,\r\n    date: centralRecord.date,\r\n    crc32: centralRecord.crc32,\r\n    compressionMethod: centralRecord.compressionMethod,\r\n    compressedLength: centralRecord.compressedLength,\r\n    uncompressedLength: centralRecord.uncompressedLength,\r\n    content: localRecord.content,\r\n    comment: centralRecord.comment};\r\n\r\n }\r\n\r\n/**\r\n * @param {CompressedFile} file\r\n * @returns {UncompressedFile}\r\n */\r\nfunction decompressFile(file) {\r\n  if( file.compressionMethod!==  compression.STORE) {\r\n    throw new Error(\r\n       \`Cannot find decompressor for compression method \${q(\r\n        file.compressionMethod)\r\n        } for file \${file.name}\`);\r\n\r\n   }\r\n  return {\r\n    name: file.name,\r\n    mode: file.mode,\r\n    date: file.date,\r\n    content: file.content,\r\n    comment: file.comment};\r\n\r\n }\r\n\r\n/**\r\n * @param {UncompressedFile} file\r\n * @returns {ArchivedFile}\r\n */\r\nfunction decodeFile(file) {\r\n  const name=  textDecoder.decode(file.name);\r\n  const comment=  textDecoder.decode(file.comment);\r\n  return {\r\n    name,\r\n    type: 'file',\r\n    mode: file.mode&  0o777,\r\n    date: file.date,\r\n    content: file.content,\r\n    comment};\r\n\r\n }\r\n\r\n/**\r\n * @param {BufferReader} reader\r\n * @param {string} name\r\n */\r\nfunction        readZip(reader, name=  '<unknown>') {\r\n  const locator=  readEndOfCentralDirectoryRecord(reader);\r\n  const centralRecords=  readCentralDirectory(reader, locator);\r\n  const localRecords=  readLocalFiles(reader, centralRecords);\r\n  const files=  new Map();\r\n\r\n  for( let i=  0; i<  centralRecords.length; i+=  1) {\r\n    const centralRecord=  centralRecords[i];\r\n    const localRecord=  localRecords[i];\r\n\r\n    checkRecords(centralRecord, localRecord, name);\r\n\r\n    if( isEncrypted(centralRecord.bitFlag)) {\r\n      throw new Error('Encrypted zip are not supported');\r\n     }\r\n\r\n    const isDir=  (centralRecord.externalFileAttributes&  0x0010)!==  0;\r\n    if( !isDir) {\r\n      const compressedFile=  recordToFile(centralRecord, localRecord);\r\n      const decompressedFile=  decompressFile(compressedFile);\r\n      const decodedFile=  decodeFile(decompressedFile);\r\n      files.set(decodedFile.name, decodedFile);\r\n     }\r\n    // TODO handle explicit directory entries\r\n   }\r\n  return files;\r\n }\r\n})\r\n\r\n,// === functors[20] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let BufferReader,readZipFormat;$h‍_imports([["./buffer-reader.js", [["BufferReader", [$h‍_a => (BufferReader = $h‍_a)]]]],["./format-reader.js", [["readZip", [$h‍_a => (readZipFormat = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n\r\nclass        ZipReader {\r\n  /**\r\n   * @param {Uint8Array} data\r\n   * @param {object} [options]\r\n   * @param {string} [options.name]\r\n   */\r\n             constructor(data,options={}){\r\n    const { name=  '<unknown>'}=   options;\r\n    const reader=  new BufferReader(data);\r\n    this.files=  readZipFormat(reader);\r\n    this.name=  name;\r\n   }\r\n\r\n  /**\r\n   * @param {string} name\r\n   * @returns {Uint8Array}\r\n   */\r\n      read(name){\r\n    const file=  this.files.get(name);\r\n    if( file===  undefined) {\r\n      throw new Error( \`Cannot find file \${name} in Zip file \${this.name}\`);\r\n     }\r\n    return file.content;\r\n   }\r\n\r\n  /**\r\n   * @param {string} name\r\n   * @returns {__import__('./types.js').ArchivedStat=}\r\n   */\r\n      stat(name){\r\n    const file=  this.files.get(name);\r\n    if( file===  undefined) {\r\n      return undefined;\r\n     }\r\n    return {\r\n      type: file.type,\r\n      mode: file.mode,\r\n      date: file.date,\r\n      comment: file.comment};\r\n\r\n   }}\r\n\r\n\r\n/**\r\n * @param {Uint8Array} data\r\n * @param {string} location\r\n * @returns {Promise<__import__('./types.js').ArchiveReader>}\r\n */$h‍_once. ZipReader(ZipReader);\r\nconst        readZip=  async( data, location)=>  {\r\n  const reader=  new ZipReader(data, { name: location});\r\n  /** @type {__import__('./types.js').ReadFn} */\r\n  const read=  async(path)=> reader.read(path);\r\n  return { read};\r\n };$h‍_once.readZip(readZip);\r\n})\r\n\r\n,// === functors[21] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([]);   // @ts-check\r\n/* eslint no-bitwise: ["off"] */\r\n\r\n/**\r\n * @type {WeakMap<BufferWriter, {\r\n *   length: number,\r\n *   index: number,\r\n *   bytes: Uint8Array,\r\n *   data: DataView,\r\n *   capacity: number,\r\n * }>}\r\n */\r\nconst privateFields=  new WeakMap();\r\n\r\n/**\r\n * @param {BufferWriter} self\r\n */\r\nconst getPrivateFields=  (self)=>{\r\n  const fields=  privateFields.get(self);\r\n  if( !fields) {\r\n    throw new Error('BufferWriter fields are not initialized');\r\n   }\r\n  return fields;\r\n };\r\n\r\nconst assertNatNumber=  (n)=>{\r\n  if( Number.isSafeInteger(n)&&  /** @type {number} */  n>=   0) {\r\n    return;\r\n   }\r\n  throw TypeError( \`must be a non-negative integer, got \${n}\`);\r\n };\r\n\r\nclass        BufferWriter {\r\n  /**\r\n   * @returns {number}\r\n   */\r\n            get length(){\r\n    return getPrivateFields(this).length;\r\n   }\r\n\r\n  /**\r\n   * @returns {number}\r\n   */\r\n           get index(){\r\n    return getPrivateFields(this).index;\r\n   }\r\n\r\n  /**\r\n   * @param {number} index\r\n   */\r\n           set index(index){\r\n    this.seek(index);\r\n   }\r\n\r\n  /**\r\n   * @param {number=} capacity\r\n   */\r\n             constructor(capacity=16){\r\n    const bytes=  new Uint8Array(capacity);\r\n    const data=  new DataView(bytes.buffer);\r\n    privateFields.set(this, {\r\n      bytes,\r\n      data,\r\n      index: 0,\r\n      length: 0,\r\n      capacity});\r\n\r\n   }\r\n\r\n  /**\r\n   * @param {number} required\r\n   */\r\n               ensureCanSeek(required){\r\n    assertNatNumber(required);\r\n    const fields=  getPrivateFields(this);\r\n    let capacity=  fields.capacity;\r\n    while( capacity<  required) {\r\n      capacity*=  2;\r\n     }\r\n    const bytes=  new Uint8Array(capacity);\r\n    const data=  new DataView(bytes.buffer);\r\n    bytes.set(fields.bytes.subarray(0, fields.length));\r\n    fields.bytes=  bytes;\r\n    fields.data=  data;\r\n    fields.capacity=  capacity;\r\n   }\r\n\r\n  /**\r\n   * @param {number} index\r\n   */\r\n      seek(index){\r\n    const fields=  getPrivateFields(this);\r\n    this.ensureCanSeek(index);\r\n    fields.index=  index;\r\n    fields.length=  Math.max(fields.index, fields.length);\r\n   }\r\n\r\n  /**\r\n   * @param {number} size\r\n   */\r\n                ensureCanWrite(size){\r\n    assertNatNumber(size);\r\n    const fields=  getPrivateFields(this);\r\n    this.ensureCanSeek(fields.index+  size);\r\n   }\r\n\r\n  /**\r\n   * @param {Uint8Array} bytes\r\n   */\r\n       write(bytes){\r\n    const fields=  getPrivateFields(this);\r\n    this.ensureCanWrite(bytes.byteLength);\r\n    fields.bytes.set(bytes, fields.index);\r\n    fields.index+=  bytes.byteLength;\r\n    fields.length=  Math.max(fields.index, fields.length);\r\n   }\r\n\r\n  /**\r\n   * @param {number} start\r\n   * @param {number} end\r\n   */\r\n           writeCopy(start,end){\r\n    assertNatNumber(start);\r\n    assertNatNumber(end);\r\n    const fields=  getPrivateFields(this);\r\n    const size=  end-  start;\r\n    this.ensureCanWrite(size);\r\n    fields.bytes.copyWithin(fields.index, start, end);\r\n    fields.index+=  size;\r\n    fields.length=  Math.max(fields.index, fields.length);\r\n   }\r\n\r\n  /**\r\n   * @param {number} value\r\n   */\r\n            writeUint8(value){\r\n    const fields=  getPrivateFields(this);\r\n    this.ensureCanWrite(1);\r\n    fields.data.setUint8(fields.index, value);\r\n    fields.index+=  1;\r\n    fields.length=  Math.max(fields.index, fields.length);\r\n   }\r\n\r\n  /**\r\n   * @param {number} value\r\n   * @param {boolean=} littleEndian\r\n   */\r\n             writeUint16(value,littleEndian){\r\n    const fields=  getPrivateFields(this);\r\n    this.ensureCanWrite(2);\r\n    const index=  fields.index;\r\n    fields.data.setUint16(index, value, littleEndian);\r\n    fields.index+=  2;\r\n    fields.length=  Math.max(fields.index, fields.length);\r\n   }\r\n\r\n  /**\r\n   * @param {number} value\r\n   * @param {boolean=} littleEndian\r\n   */\r\n             writeUint32(value,littleEndian){\r\n    const fields=  getPrivateFields(this);\r\n    this.ensureCanWrite(4);\r\n    const index=  fields.index;\r\n    fields.data.setUint32(index, value, littleEndian);\r\n    fields.index+=  4;\r\n    fields.length=  Math.max(fields.index, fields.length);\r\n   }\r\n\r\n  /**\r\n   * @param {number=} begin\r\n   * @param {number=} end\r\n   * @returns {Uint8Array}\r\n   */\r\n          subarray(begin,end){\r\n    const fields=  getPrivateFields(this);\r\n    return fields.bytes.subarray(0, fields.length).subarray(begin, end);\r\n   }\r\n\r\n  /**\r\n   * @param {number=} begin\r\n   * @param {number=} end\r\n   * @returns {Uint8Array}\r\n   */\r\n       slice(begin,end){\r\n    return this.subarray(begin, end).slice();\r\n   }}$h‍_once.BufferWriter(BufferWriter);\r\n})\r\n\r\n,// === functors[22] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let crc32,signature,compression;$h‍_imports([["./crc32.js", [["crc32", [$h‍_a => (crc32 = $h‍_a)]]]],["./signature.js", [["*", [$h‍_a => (signature = $h‍_a)]]]],["./compression.js", [["*", [$h‍_a => (compression = $h‍_a)]]]]]);Object.defineProperty(writeZipRecords, 'name', {value: "writeZipRecords"});$h‍_once.writeZipRecords(writeZipRecords);Object.defineProperty(writeZip, 'name', {value: "writeZip"});$h‍_once.writeZip(writeZip);   \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nconst UNIX=  3;\r\nconst UNIX_VERSION=  30;\r\n\r\nconst textEncoder=  new TextEncoder();\r\n\r\n/**\r\n * @param {BufferWriter} writer\r\n * @param {Date?} date\r\n * @see http://www.delorie.com/djgpp/doc/rbinter/it/65/16.html\r\n * @see http://www.delorie.com/djgpp/doc/rbinter/it/66/16.html\r\n */\r\nfunction writeDosDateTime(writer, date) {\r\n  const dosTime=\r\n    date!==  undefined&&  date!==  null?\r\n         ( date.getUTCFullYear()-  1980&   0x7f)<<  25|   // year\r\n          date.getUTCMonth()+  1<<   21|   // month\r\n         date.getUTCDate()<<  16|   // day\r\n         date.getUTCHours()<<  11|   // hour\r\n         date.getUTCMinutes()<<  5|   // minute\r\n         date.getUTCSeconds()>>  1  // second\r\n:       0; // Epoch origin by default.\r\n  writer.writeUint32(dosTime, true);\r\n }\r\n\r\n/**\r\n * @param {BufferWriter} writer\r\n * @param {FileRecord} file\r\n * @returns {LocalFileLocator}\r\n */\r\nfunction writeFile(writer, file) {\r\n  // Header\r\n  const fileStart=  writer.index;\r\n  writer.write(signature.LOCAL_FILE_HEADER);\r\n  const headerStart=  writer.index;\r\n  // Version needed to extract\r\n  writer.writeUint16(10, true);\r\n  writer.writeUint16(file.bitFlag, true);\r\n  writer.writeUint16(file.compressionMethod, true);\r\n  writeDosDateTime(writer, file.date);\r\n  writer.writeUint32(file.crc32, true);\r\n  writer.writeUint32(file.compressedLength, true);\r\n  writer.writeUint32(file.uncompressedLength, true);\r\n  writer.writeUint16(file.name.length, true);\r\n  const headerEnd=  writer.length;\r\n\r\n  // TODO count of extra fields length\r\n  writer.writeUint16(0, true);\r\n  writer.write(file.name);\r\n  // TODO write extra fields\r\n  writer.write(file.content);\r\n\r\n  return {\r\n    fileStart,\r\n    headerStart,\r\n    headerEnd};\r\n\r\n }\r\n\r\n/**\r\n * @param {BufferWriter} writer\r\n * @param {FileRecord} file\r\n * @param {LocalFileLocator} locator\r\n */\r\nfunction writeCentralFileHeader(writer, file, locator) {\r\n  writer.write(signature.CENTRAL_FILE_HEADER);\r\n  writer.writeUint8(file.version);\r\n  writer.writeUint8(file.madeBy);\r\n  writer.writeCopy(locator.headerStart, locator.headerEnd);\r\n  // TODO extra fields length\r\n  writer.writeUint16(0, true);\r\n  writer.writeUint16(file.comment.length, true);\r\n  writer.writeUint16(file.diskNumberStart, true);\r\n  writer.writeUint16(file.internalFileAttributes, true);\r\n  writer.writeUint32(file.externalFileAttributes, true);\r\n  writer.writeUint32(locator.fileStart, true);\r\n  writer.write(file.centralName);\r\n  // TODO extra fields\r\n  writer.write(file.comment);\r\n }\r\n\r\n/**\r\n * @param {BufferWriter} writer\r\n * @param {number} entriesCount\r\n * @param {number} centralDirectoryStart\r\n * @param {number} centralDirectoryLength\r\n * @param {Uint8Array} commentBytes\r\n */\r\nfunction writeEndOfCentralDirectoryRecord(\r\n  writer,\r\n  entriesCount,\r\n  centralDirectoryStart,\r\n  centralDirectoryLength,\r\n  commentBytes)\r\n  {\r\n  writer.write(signature.CENTRAL_DIRECTORY_END);\r\n  writer.writeUint16(0, true);\r\n  writer.writeUint16(0, true);\r\n  writer.writeUint16(entriesCount, true);\r\n  writer.writeUint16(entriesCount, true);\r\n  writer.writeUint32(centralDirectoryLength, true);\r\n  writer.writeUint32(centralDirectoryStart, true);\r\n  writer.writeUint16(commentBytes.length, true);\r\n  writer.write(commentBytes);\r\n }\r\n\r\n/**\r\n * @param {BufferWriter} writer\r\n * @param {Array<FileRecord>} records\r\n * @param {string} comment\r\n */\r\nfunction        writeZipRecords(writer, records, comment=  '') {\r\n  // Write records with local headers.\r\n  const locators=  [];\r\n  for( let i=  0; i<  records.length; i+=  1) {\r\n    locators.push(writeFile(writer, records[i]));\r\n   }\r\n\r\n  // writeCentralDirectory\r\n  const centralDirectoryStart=  writer.index;\r\n  for( let i=  0; i<  locators.length; i+=  1) {\r\n    writeCentralFileHeader(writer, records[i], locators[i]);\r\n   }\r\n  const centralDirectoryLength=  writer.index-  centralDirectoryStart;\r\n\r\n  const commentBytes=  textEncoder.encode(comment);\r\n\r\n  // Write central directory end.\r\n  writeEndOfCentralDirectoryRecord(\r\n    writer,\r\n    records.length,\r\n    centralDirectoryStart,\r\n    centralDirectoryLength,\r\n    commentBytes);\r\n\r\n }\r\n\r\n/**\r\n * @param {__import__('./types.js').ArchivedFile} file\r\n * @returns {__import__('./types.js').UncompressedFile}\r\n */\r\nfunction encodeFile(file) {\r\n  const name=  textEncoder.encode(file.name.replace(/\\\\/g, '/'));\r\n  const comment=  textEncoder.encode(file.comment);\r\n  return {\r\n    name,\r\n    mode: file.mode,\r\n    date: file.date,\r\n    content: file.content,\r\n    comment};\r\n\r\n }\r\n\r\n/**\r\n * @param {__import__('./types.js').UncompressedFile} file\r\n * @returns {__import__('./types.js').CompressedFile}\r\n */\r\nfunction compressFileWithStore(file) {\r\n  return {\r\n    name: file.name,\r\n    mode: file.mode,\r\n    date: file.date,\r\n    crc32: crc32(file.content),\r\n    compressionMethod: compression.STORE,\r\n    compressedLength: file.content.length,\r\n    uncompressedLength: file.content.length,\r\n    content: file.content,\r\n    comment: file.comment};\r\n\r\n }\r\n\r\n/**\r\n * Computes Zip external file attributes field from a UNIX mode for a file.\r\n *\r\n * @param {number} mode\r\n * @returns {number}\r\n */\r\nfunction externalFileAttributes(mode) {\r\n  return ( mode&  0o777|   0o100000)<<  16;\r\n }\r\n\r\n// TODO Add support for directory records.\r\n// /**\r\n//  * @param {number} mode\r\n//  * @return {number}\r\n//  */\r\n// function externalDirectoryAttributes(mode) {\r\n//   // The 0x10 is the DOS directory attribute, which is set regardless of platform.\r\n//   return ((mode & 0o777) | 0o40000) << 16 | 0x10;\r\n// }\r\n\r\n/**\r\n * @param {__import__('./types.js').CompressedFile} file\r\n * @returns {FileRecord}\r\n */\r\nfunction makeFileRecord(file) {\r\n  return {\r\n    name: file.name,\r\n    centralName: file.name,\r\n    madeBy: UNIX,\r\n    version: UNIX_VERSION,\r\n    versionNeeded: 0, // TODO this is probably too lax.\r\n    bitFlag: 0,\r\n    compressionMethod: compression.STORE,\r\n    date: file.date,\r\n    crc32: file.crc32,\r\n    compressedLength: file.compressedLength,\r\n    uncompressedLength: file.uncompressedLength,\r\n    diskNumberStart: 0,\r\n    internalFileAttributes: 0,\r\n    externalFileAttributes: externalFileAttributes(file.mode),\r\n    comment: file.comment,\r\n    content: file.content};\r\n\r\n }\r\n\r\n/**\r\n * @param {BufferWriter} writer\r\n * @param {Array<__import__('./types.js').ArchivedFile>} files\r\n * @param {string} comment\r\n */\r\nfunction        writeZip(writer, files, comment=  '') {\r\n  const encodedFiles=  files.map(encodeFile);\r\n  const compressedFiles=  encodedFiles.map(compressFileWithStore);\r\n  // TODO collate directoryRecords from file bases.\r\n  const fileRecords=  compressedFiles.map(makeFileRecord);\r\n  writeZipRecords(writer, fileRecords, comment);\r\n }\r\n})\r\n\r\n,// === functors[23] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let BufferWriter,writeZipFormat;$h‍_imports([["./buffer-writer.js", [["BufferWriter", [$h‍_a => (BufferWriter = $h‍_a)]]]],["./format-writer.js", [["writeZip", [$h‍_a => (writeZipFormat = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n\r\nclass        ZipWriter {\r\n  /**\r\n   * @param {{\r\n   *   date: Date,\r\n   * }} options\r\n   */\r\n             constructor(options={date:new Date()}){\r\n    const { date}=   options;\r\n    /** type {Map<string, ZFile>} */\r\n    this.files=  new Map();\r\n    this.date=  date;\r\n   }\r\n\r\n  /**\r\n   * @param {string} name\r\n   * @param {Uint8Array} content\r\n   * @param {{\r\n   *   mode?: number,\r\n   *   date?: Date,\r\n   *   comment?: string,\r\n   * }} [options]\r\n   */\r\n       write(name,content,options={}){\r\n    const { mode=  0o644, date=  undefined, comment=  ''}=   options;\r\n    if( !content) {\r\n      throw new Error( \`ZipWriter write requires content for \${name}\`);\r\n     }\r\n    this.files.set(name, {\r\n      name,\r\n      mode,\r\n      date,\r\n      content,\r\n      comment});\r\n\r\n   }\r\n\r\n  /**\r\n   * @returns {Uint8Array}\r\n   */\r\n          snapshot(){\r\n    const writer=  new BufferWriter();\r\n    writeZipFormat(writer, Array.from(this.files.values()));\r\n    return writer.subarray();\r\n   }}\r\n\r\n\r\n/**\r\n * @returns {__import__('./types.js').ArchiveWriter}\r\n */$h‍_once. ZipWriter(ZipWriter);\r\nconst        writeZip=  ()=>  {\r\n  const writer=  new ZipWriter();\r\n  /** @type {__import__('./types.js').WriteFn} */\r\n  const write=  async( path, data)=>  {\r\n    writer.write(path, data);\r\n   };\r\n  /** @type {__import__('./types.js').SnapshotFn} */\r\n  const snapshot=  async()=>   writer.snapshot();\r\n  return { write, snapshot};\r\n };$h‍_once.writeZip(writeZip);\r\n})\r\n\r\n,// === functors[24] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   $h‍_imports([["./src/types.js", []],["./src/reader.js", []],["./src/writer.js", []]]);   \r\n})\r\n\r\n,// === functors[25] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let ZipReader,link,parserPreCjs,parserJson,parserText,parserBytes,parserPreMjs,parseLocatedJson,unpackReadPowers,join,assertCompartmentMap;$h‍_imports([["@endo/zip", [["ZipReader", [$h‍_a => (ZipReader = $h‍_a)]]]],["./link.js", [["link", [$h‍_a => (link = $h‍_a)]]]],["./parse-pre-cjs.js", [["default", [$h‍_a => (parserPreCjs = $h‍_a)]]]],["./parse-json.js", [["default", [$h‍_a => (parserJson = $h‍_a)]]]],["./parse-text.js", [["default", [$h‍_a => (parserText = $h‍_a)]]]],["./parse-bytes.js", [["default", [$h‍_a => (parserBytes = $h‍_a)]]]],["./parse-pre-mjs.js", [["default", [$h‍_a => (parserPreMjs = $h‍_a)]]]],["./json.js", [["parseLocatedJson", [$h‍_a => (parseLocatedJson = $h‍_a)]]]],["./powers.js", [["unpackReadPowers", [$h‍_a => (unpackReadPowers = $h‍_a)]]]],["./node-module-specifier.js", [["join", [$h‍_a => (join = $h‍_a)]]]],["./compartment-map.js", [["assertCompartmentMap", [$h‍_a => (assertCompartmentMap = $h‍_a)]]]]]);   \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nconst DefaultCompartment=  Compartment;\r\n\r\nconst { Fail, quote: q}=   assert;\r\n\r\nconst textDecoder=  new TextDecoder();\r\n\r\nconst { freeze}=   Object;\r\n\r\n/** @type {ParserForLanguage} */\r\nconst parserForLanguage=  {\r\n  'pre-cjs-json': parserPreCjs,\r\n  'pre-mjs-json': parserPreMjs,\r\n  json: parserJson,\r\n  text: parserText,\r\n  bytes: parserBytes};\r\n\r\n\r\n/**\r\n * @param {string} errorMessage - error to throw on execute\r\n * @returns {StaticModuleType}\r\n */\r\nconst postponeErrorToExecute=  (errorMessage)=>{\r\n  // Return a place-holder that'd throw an error if executed\r\n  // This allows cjs parser to more eagerly find calls to require\r\n  // - if parser identified a require call that's a local function, execute will never be called\r\n  // - if actual required module is missing, the error will happen anyway - at execution time\r\n\r\n  const record=  freeze({\r\n    imports: [],\r\n    exports: [],\r\n    execute: ()=>  {\r\n      throw Error(errorMessage);\r\n     }});\r\n\r\n\r\n  return record;\r\n };\r\n\r\n/**\r\n * @callback ArchiveImportHookMaker\r\n * @param {string} packageLocation\r\n * @param {string} packageName\r\n * @returns {ImportHook}\r\n */\r\n\r\n/**\r\n * @param {(path: string) => Promise<Uint8Array>} get\r\n * @param {Record<string, CompartmentDescriptor>} compartments\r\n * @param {string} archiveLocation\r\n * @param {HashFn} [computeSha512]\r\n * @param {ComputeSourceLocationHook} [computeSourceLocation]\r\n * @returns {ArchiveImportHookMaker}\r\n */\r\nconst        makeArchiveImportHookMaker=  (\r\n  get,\r\n  compartments,\r\n  archiveLocation,\r\n  computeSha512=  undefined,\r\n  computeSourceLocation=  undefined)=>\r\n     {\r\n  // per-assembly:\r\n  /** @type {ArchiveImportHookMaker} */\r\n  const makeImportHook=  (packageLocation, packageName)=>  {\r\n    // per-compartment:\r\n    const { modules}=   compartments[packageLocation];\r\n    /** @type {ImportHook} */\r\n    const importHook=  async(moduleSpecifier)=> {\r\n      // per-module:\r\n      const module=  modules[moduleSpecifier];\r\n      if( module===  undefined) {\r\n        throw new Error(\r\n           \`Cannot find module \${q(moduleSpecifier)} in package \${q(\r\n            packageLocation)\r\n            } in archive \${q(archiveLocation)}\`);\r\n\r\n       }\r\n      if( module.deferredError!==  undefined) {\r\n        return postponeErrorToExecute(module.deferredError);\r\n       }\r\n      if( module.parser===  undefined) {\r\n        throw new Error(\r\n           \`Cannot parse module \${q(moduleSpecifier)} in package \${q(\r\n            packageLocation)\r\n            } in archive \${q(archiveLocation)}\`);\r\n\r\n       }\r\n      if( parserForLanguage[module.parser]===  undefined) {\r\n        throw new Error(\r\n           \`Cannot parse \${q(module.parser)} module \${q(\r\n            moduleSpecifier)\r\n            } in package \${q(packageLocation)} in archive \${q(archiveLocation)}\`);\r\n\r\n       }\r\n      const { parse}=   parserForLanguage[module.parser];\r\n      const moduleLocation=   \`\${packageLocation}/\${module.location}\`;\r\n      const moduleBytes=  await get(moduleLocation);\r\n\r\n      if( computeSha512!==  undefined&&  module.sha512!==  undefined) {\r\n        const sha512=  computeSha512(moduleBytes);\r\n        if( sha512!==  module.sha512) {\r\n          throw new Error(\r\n             \`Module \${q(module.location)} of package \${q(\r\n              packageLocation)\r\n              } in archive \${q(\r\n              archiveLocation)\r\n              } failed a SHA-512 integrity check\`);\r\n\r\n         }\r\n       }\r\n\r\n      let sourceLocation=   \`file:///\${moduleLocation}\`;\r\n      if( packageName!==  undefined) {\r\n        const base=  packageName.split('/').slice(-1).join('/');\r\n        sourceLocation=   \`.../\${join(base,moduleSpecifier) }\`;\r\n       }\r\n      if( computeSourceLocation!==  undefined) {\r\n        sourceLocation=\r\n          computeSourceLocation(packageLocation, moduleSpecifier)||\r\n          sourceLocation;\r\n       }\r\n\r\n      // eslint-disable-next-line no-await-in-loop\r\n      const { record}=   await parse(\r\n        moduleBytes,\r\n        moduleSpecifier,\r\n        sourceLocation,\r\n        packageLocation);\r\n\r\n      return { record, specifier: moduleSpecifier};\r\n     };\r\n    return importHook;\r\n   };\r\n  return makeImportHook;\r\n };$h‍_once.makeArchiveImportHookMaker(makeArchiveImportHookMaker);\r\n\r\nconst makeFeauxModuleExportsNamespace=  (Compartment)=>{\r\n  // @ts-ignore Unclear at time of writing why Compartment type is not\r\n  // constructible.\r\n  const compartment=  new Compartment(\r\n    {},\r\n    {},\r\n    {\r\n      resolveHook() {\r\n        return '.';\r\n       },\r\n      importHook() {\r\n        return {\r\n          imports: [],\r\n          execute() { }};\r\n\r\n       }});\r\n\r\n\r\n  return compartment.module('.');\r\n };\r\n\r\n/**\r\n * @param {Uint8Array} archiveBytes\r\n * @param {string} [archiveLocation]\r\n * @param {object} [options]\r\n * @param {string} [options.expectedSha512]\r\n * @param {HashFn} [options.computeSha512]\r\n * @param {Record<string, unknown>} [options.modules]\r\n * @param {Compartment} [options.Compartment]\r\n * @param {ComputeSourceLocationHook} [options.computeSourceLocation]\r\n * @returns {Promise<Application>}\r\n */\r\nconst        parseArchive=  async(\r\n  archiveBytes,\r\n  archiveLocation=  '<unknown>',\r\n  options=  {})=>\r\n     {\r\n  const {\r\n    computeSha512=  undefined,\r\n    expectedSha512=  undefined,\r\n    computeSourceLocation=  undefined,\r\n    Compartment=  DefaultCompartment,\r\n    modules=  undefined}=\r\n      options;\r\n\r\n  const archive=  new ZipReader(archiveBytes, { name: archiveLocation});\r\n\r\n  // Track all modules that get loaded, all files that are used.\r\n  const unseen=  new Set(archive.files.keys());\r\n  unseen.size>=  2||\r\n    Fail \`Archive failed sanity check: should contain at least a compartment map file and one module file in \${q(\r\n      archiveLocation)\r\n      }\`;\r\n\r\n  /**\r\n   * @param {string} path\r\n   */\r\n  const get=  async(path)=> {\r\n    unseen.delete(path);\r\n    return archive.read(path);\r\n   };\r\n\r\n  const compartmentMapBytes=  await get('compartment-map.json');\r\n\r\n  let sha512;\r\n  if( computeSha512!==  undefined) {\r\n    sha512=  computeSha512(compartmentMapBytes);\r\n   }\r\n  if( expectedSha512!==  undefined) {\r\n    if( sha512===  undefined) {\r\n      throw new Error(\r\n         \`Cannot verify expectedSha512 without also providing computeSha512, for archive \${archiveLocation}\`);\r\n\r\n     }\r\n    if( sha512!==  expectedSha512) {\r\n      throw new Error(\r\n         \`Archive compartment map failed a SHA-512 integrity check, expected \${expectedSha512}, got \${sha512}, for archive \${archiveLocation}\`);\r\n\r\n     }\r\n   }\r\n  const compartmentMapText=  textDecoder.decode(compartmentMapBytes);\r\n  const compartmentMap=  parseLocatedJson(\r\n    compartmentMapText,\r\n    'compartment-map.json');\r\n\r\n  assertCompartmentMap(compartmentMap, archiveLocation);\r\n\r\n  const {\r\n    compartments,\r\n    entry: { module: moduleSpecifier}}=\r\n      compartmentMap;\r\n\r\n  // Archive integrity checks: ensure every module is pre-loaded so its hash\r\n  // gets checked, and ensure that every file in the archive is used, and\r\n  // therefore checked.\r\n  if( computeSha512!==  undefined) {\r\n    const makeImportHook=  makeArchiveImportHookMaker(\r\n      get,\r\n      compartments,\r\n      archiveLocation,\r\n      computeSha512,\r\n      computeSourceLocation);\r\n\r\n    // A weakness of the current Compartment design is that the \`modules\` map\r\n    // must be given a module namespace object that passes a brand check.\r\n    // We don't have module instances for the preload phase, so we supply fake\r\n    // namespaces.\r\n    const { compartment, pendingJobsPromise}=   link(compartmentMap, {\r\n      makeImportHook,\r\n      parserForLanguage,\r\n      modules: Object.fromEntries(\r\n        Object.keys(modules||  {}).map((specifier)=>{\r\n          return [specifier, makeFeauxModuleExportsNamespace(Compartment)];\r\n         })),\r\n\r\n      Compartment});\r\n\r\n\r\n    await pendingJobsPromise;\r\n\r\n    await compartment.load(moduleSpecifier);\r\n    unseen.size===  0||\r\n      Fail \`Archive contains extraneous files: \${q([...unseen])} in \${q(\r\n        archiveLocation)\r\n        }\`;\r\n   }\r\n\r\n  /** @type {ExecuteFn} */\r\n  const execute=  async(options)=> {\r\n    const { globals, modules, transforms, __shimTransforms__, Compartment}=\r\n      options||  {};\r\n    const makeImportHook=  makeArchiveImportHookMaker(\r\n      get,\r\n      compartments,\r\n      archiveLocation,\r\n      computeSha512,\r\n      computeSourceLocation);\r\n\r\n    const { compartment, pendingJobsPromise}=   link(compartmentMap, {\r\n      makeImportHook,\r\n      parserForLanguage,\r\n      globals,\r\n      modules,\r\n      transforms,\r\n      __shimTransforms__,\r\n      Compartment});\r\n\r\n\r\n    await pendingJobsPromise;\r\n\r\n    // eslint-disable-next-line dot-notation\r\n    return compartment['import'](moduleSpecifier);\r\n   };\r\n\r\n  return { import: execute, sha512};\r\n };\r\n\r\n/**\r\n * @param {ReadFn | ReadPowers} readPowers\r\n * @param {string} archiveLocation\r\n * @param {LoadArchiveOptions} [options]\r\n * @returns {Promise<Application>}\r\n */$h‍_once.parseArchive(parseArchive);\r\nconst        loadArchive=  async(\r\n  readPowers,\r\n  archiveLocation,\r\n  options=  {})=>\r\n     {\r\n  const { read, computeSha512}=   unpackReadPowers(readPowers);\r\n  const { expectedSha512, computeSourceLocation, modules}=   options;\r\n  const archiveBytes=  await read(archiveLocation);\r\n  return parseArchive(archiveBytes, archiveLocation, {\r\n    computeSha512,\r\n    expectedSha512,\r\n    computeSourceLocation,\r\n    modules});\r\n\r\n };\r\n\r\n/**\r\n * @param {ReadFn | ReadPowers} readPowers\r\n * @param {string} archiveLocation\r\n * @param {ExecuteOptions & LoadArchiveOptions} options\r\n * @returns {Promise<object>}\r\n */$h‍_once.loadArchive(loadArchive);\r\nconst        importArchive=  async( readPowers, archiveLocation, options)=>  {\r\n  const archive=  await loadArchive(readPowers, archiveLocation, options);\r\n  return archive.import(options);\r\n };$h‍_once.importArchive(importArchive);\r\n})\r\n\r\n,// === functors[26] ===\r\n(({   imports: $h‍_imports,   liveVar: $h‍_live,   onceVar: $h‍_once,   importMeta: $h‍____meta,  }) => {   let link,makeArchiveImportHookMaker;$h‍_imports([["./link.js", [["link", [$h‍_a => (link = $h‍_a)]]]],["./import-archive.js", [["makeArchiveImportHookMaker", [$h‍_a => (makeArchiveImportHookMaker = $h‍_a)]]]]]);Object.defineProperty(loadApplication, 'name', {value: "loadApplication"});$h‍_once.loadApplication(loadApplication);   \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nconst textEncoder=  new TextEncoder();\r\n\r\nfunction        loadApplication(\r\n  compartmentMap,\r\n  lookupModule,\r\n  archiveLocation)\r\n  {\r\n  const _lookupModule=  async(moduleLocation)=>\r\n    textEncoder.encode(JSON.stringify(await lookupModule(moduleLocation)));\r\n\r\n  const {\r\n    compartments: compartmentDescriptors,\r\n    entry: { module: entrySpecifier}}=\r\n      compartmentMap;\r\n\r\n  const archiveMakeImportHook=  makeArchiveImportHookMaker(\r\n    _lookupModule, // <-- this is our get function\r\n    compartmentDescriptors,\r\n    archiveLocation);\r\n\r\n\r\n  const { compartment: entryCompartment, compartments}=   link(compartmentMap, {\r\n    makeImportHook: archiveMakeImportHook,\r\n    globals: globalThis\r\n    // transforms,\r\n});\r\n\r\n  /** @type {ExecuteFn} */\r\n  const execute=  ()=>  {\r\n    // eslint-disable-next-line dot-notation\r\n    return entryCompartment['import'](entrySpecifier);\r\n   };\r\n\r\n  return { execute, compartments};\r\n }\r\n})\r\n\r\n]; // functors end\r\n\r\n  const cell = (name, value = undefined) => {\r\n    const observers = [];\r\n    return Object.freeze({\r\n      get: Object.freeze(() => {\r\n        return value;\r\n      }),\r\n      set: Object.freeze((newValue) => {\r\n        value = newValue;\r\n        for (const observe of observers) {\r\n          observe(value);\r\n        }\r\n      }),\r\n      observe: Object.freeze((observe) => {\r\n        observers.push(observe);\r\n        observe(value);\r\n      }),\r\n      enumerable: true,\r\n    });\r\n  };\r\n\r\n  const cells = [\r\n    {\r\n      policyLookupHelper: cell("policyLookupHelper"),\r\n      isAllowingEverything: cell("isAllowingEverything"),\r\n      isAttenuationDefinition: cell("isAttenuationDefinition"),\r\n      getAttenuatorFromDefinition: cell("getAttenuatorFromDefinition"),\r\n      assertPackagePolicy: cell("assertPackagePolicy"),\r\n      assertPolicy: cell("assertPolicy"),\r\n    },\r\n    {\r\n      stringCompare: cell("stringCompare"),\r\n      pathCompare: cell("pathCompare"),\r\n      assertCompartmentMap: cell("assertCompartmentMap"),\r\n    },\r\n    {\r\n      parseLocatedJson: cell("parseLocatedJson"),\r\n    },\r\n    {\r\n      parseExtension: cell("parseExtension"),\r\n    },\r\n    {\r\n      resolve: cell("resolve"),\r\n      join: cell("join"),\r\n      relativize: cell("relativize"),\r\n    },\r\n    {\r\n      ATTENUATORS_COMPARTMENT: cell("ATTENUATORS_COMPARTMENT"),\r\n      detectAttenuators: cell("detectAttenuators"),\r\n      dependencyAllowedByPolicy: cell("dependencyAllowedByPolicy"),\r\n      getPolicyForPackage: cell("getPolicyForPackage"),\r\n      makeDeferredAttenuatorsProvider: cell("makeDeferredAttenuatorsProvider"),\r\n      attenuateGlobals: cell("attenuateGlobals"),\r\n      enforceModulePolicy: cell("enforceModulePolicy"),\r\n      attenuateModuleHook: cell("attenuateModuleHook"),\r\n      diagnoseMissingCompartmentError: cell("diagnoseMissingCompartmentError"),\r\n    },\r\n    {\r\n      mapParsers: cell("mapParsers"),\r\n      link: cell("link"),\r\n      assemble: cell("assemble"),\r\n    },\r\n    {\r\n      default: cell("default"),\r\n      parseBytes: cell("parseBytes"),\r\n    },\r\n    {\r\n      default: cell("default"),\r\n      parseJson: cell("parseJson"),\r\n    },\r\n    {\r\n      getModulePaths: cell("getModulePaths"),\r\n      wrap: cell("wrap"),\r\n    },\r\n    {\r\n      default: cell("default"),\r\n      parsePreCjs: cell("parsePreCjs"),\r\n    },\r\n    {\r\n      default: cell("default"),\r\n      parsePreMjs: cell("parsePreMjs"),\r\n    },\r\n    {\r\n      default: cell("default"),\r\n      parseText: cell("parseText"),\r\n    },\r\n    {\r\n      unpackReadPowers: cell("unpackReadPowers"),\r\n    },\r\n    {\r\n      BufferReader: cell("BufferReader"),\r\n    },\r\n    {\r\n      STORE: cell("STORE"),\r\n    },\r\n    {\r\n      crc32: cell("crc32"),\r\n    },\r\n    {\r\n      LOCAL_FILE_HEADER: cell("LOCAL_FILE_HEADER"),\r\n      CENTRAL_FILE_HEADER: cell("CENTRAL_FILE_HEADER"),\r\n      CENTRAL_DIRECTORY_END: cell("CENTRAL_DIRECTORY_END"),\r\n      ZIP64_CENTRAL_DIRECTORY_LOCATOR: cell("ZIP64_CENTRAL_DIRECTORY_LOCATOR"),\r\n      ZIP64_CENTRAL_DIRECTORY_END: cell("ZIP64_CENTRAL_DIRECTORY_END"),\r\n      DATA_DESCRIPTOR: cell("DATA_DESCRIPTOR"),\r\n    },\r\n    {\r\n    },\r\n    {\r\n      readZip: cell("readZip"),\r\n    },\r\n    {\r\n      ZipReader: cell("ZipReader"),\r\n      readZip: cell("readZip"),\r\n    },\r\n    {\r\n      BufferWriter: cell("BufferWriter"),\r\n    },\r\n    {\r\n      writeZipRecords: cell("writeZipRecords"),\r\n      writeZip: cell("writeZip"),\r\n    },\r\n    {\r\n      ZipWriter: cell("ZipWriter"),\r\n      writeZip: cell("writeZip"),\r\n    },\r\n    {\r\n      ZipReader: cell("ZipReader"),\r\n      readZip: cell("readZip"),\r\n      ZipWriter: cell("ZipWriter"),\r\n      writeZip: cell("writeZip"),\r\n    },\r\n    {\r\n      makeArchiveImportHookMaker: cell("makeArchiveImportHookMaker"),\r\n      parseArchive: cell("parseArchive"),\r\n      loadArchive: cell("loadArchive"),\r\n      importArchive: cell("importArchive"),\r\n    },\r\n    {\r\n      loadApplication: cell("loadApplication"),\r\n    },\r\n  ];\r\n\r\n  Object.defineProperties(cells[24], Object.getOwnPropertyDescriptors(cells[18]));\r\n\r\n  Object.defineProperties(cells[24], {"ZipReader": { value: cells[20]["ZipReader"] },"readZip": { value: cells[20]["readZip"] },"ZipWriter": { value: cells[23]["ZipWriter"] },"writeZip": { value: cells[23]["writeZip"] } });\r\n\r\n  const namespaces = cells.map(cells => Object.freeze(Object.create(null, cells)));\r\n\r\n  for (let index = 0; index < namespaces.length; index += 1) {\r\n    cells[index]['*'] = cell('*', namespaces[index]);\r\n  }\r\n\r\nfunction observeImports(map, importName, importIndex) {\r\n  for (const [name, observers] of map.get(importName)) {\r\n    const cell = cells[importIndex][name];\r\n    if (cell === undefined) {\r\n      throw new ReferenceError(\`Cannot import name \${name}\`);\r\n    }\r\n    for (const observer of observers) {\r\n      cell.observe(observer);\r\n    }\r\n  }\r\n}\r\n\r\n\r\n  functors[0]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      policyLookupHelper: cells[0].policyLookupHelper.set,\r\n      isAllowingEverything: cells[0].isAllowingEverything.set,\r\n      isAttenuationDefinition: cells[0].isAttenuationDefinition.set,\r\n      getAttenuatorFromDefinition: cells[0].getAttenuatorFromDefinition.set,\r\n      assertPackagePolicy: cells[0].assertPackagePolicy.set,\r\n      assertPolicy: cells[0].assertPolicy.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[1]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./policy-format.js", 0);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      stringCompare: cells[1].stringCompare.set,\r\n      pathCompare: cells[1].pathCompare.set,\r\n      assertCompartmentMap: cells[1].assertCompartmentMap.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[2]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      parseLocatedJson: cells[2].parseLocatedJson.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[3]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      parseExtension: cells[3].parseExtension.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[4]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      resolve: cells[4].resolve.set,\r\n      join: cells[4].join.set,\r\n      relativize: cells[4].relativize.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[5]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./policy-format.js", 0);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      ATTENUATORS_COMPARTMENT: cells[5].ATTENUATORS_COMPARTMENT.set,\r\n      detectAttenuators: cells[5].detectAttenuators.set,\r\n      dependencyAllowedByPolicy: cells[5].dependencyAllowedByPolicy.set,\r\n      getPolicyForPackage: cells[5].getPolicyForPackage.set,\r\n      makeDeferredAttenuatorsProvider: cells[5].makeDeferredAttenuatorsProvider.set,\r\n      attenuateGlobals: cells[5].attenuateGlobals.set,\r\n      enforceModulePolicy: cells[5].enforceModulePolicy.set,\r\n      attenuateModuleHook: cells[5].attenuateModuleHook.set,\r\n      diagnoseMissingCompartmentError: cells[5].diagnoseMissingCompartmentError.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[6]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./extension.js", 3);\r\n      observeImports(map, "./node-module-specifier.js", 4);\r\n      observeImports(map, "./policy.js", 5);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      mapParsers: cells[6].mapParsers.set,\r\n      link: cells[6].link.set,\r\n      assemble: cells[6].assemble.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[7]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      default: cells[7].default.set,\r\n      parseBytes: cells[7].parseBytes.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[8]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./json.js", 2);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      default: cells[8].default.set,\r\n      parseJson: cells[8].parseJson.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[9]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      getModulePaths: cells[9].getModulePaths.set,\r\n      wrap: cells[9].wrap.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[10]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./json.js", 2);\r\n      observeImports(map, "./parse-cjs-shared-export-wrapper.js", 9);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      default: cells[10].default.set,\r\n      parsePreCjs: cells[10].parsePreCjs.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[11]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./json.js", 2);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      default: cells[11].default.set,\r\n      parsePreMjs: cells[11].parsePreMjs.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[12]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      default: cells[12].default.set,\r\n      parseText: cells[12].parseText.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[13]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      unpackReadPowers: cells[13].unpackReadPowers.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[14]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      BufferReader: cells[14].BufferReader.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[15]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      STORE: cells[15].STORE.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[16]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      crc32: cells[16].crc32.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[17]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      LOCAL_FILE_HEADER: cells[17].LOCAL_FILE_HEADER.set,\r\n      CENTRAL_FILE_HEADER: cells[17].CENTRAL_FILE_HEADER.set,\r\n      CENTRAL_DIRECTORY_END: cells[17].CENTRAL_DIRECTORY_END.set,\r\n      ZIP64_CENTRAL_DIRECTORY_LOCATOR: cells[17].ZIP64_CENTRAL_DIRECTORY_LOCATOR.set,\r\n      ZIP64_CENTRAL_DIRECTORY_END: cells[17].ZIP64_CENTRAL_DIRECTORY_END.set,\r\n      DATA_DESCRIPTOR: cells[17].DATA_DESCRIPTOR.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[18]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[19]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./compression.js", 15);\r\n      observeImports(map, "./crc32.js", 16);\r\n      observeImports(map, "./signature.js", 17);\r\n      observeImports(map, "./types.js", 18);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      readZip: cells[19].readZip.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[20]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./buffer-reader.js", 14);\r\n      observeImports(map, "./format-reader.js", 19);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      ZipReader: cells[20].ZipReader.set,\r\n      readZip: cells[20].readZip.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[21]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      BufferWriter: cells[21].BufferWriter.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[22]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./compression.js", 15);\r\n      observeImports(map, "./crc32.js", 16);\r\n      observeImports(map, "./signature.js", 17);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      writeZipRecords: cells[22].writeZipRecords.set,\r\n      writeZip: cells[22].writeZip.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[23]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./buffer-writer.js", 21);\r\n      observeImports(map, "./format-writer.js", 22);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      ZipWriter: cells[23].ZipWriter.set,\r\n      writeZip: cells[23].writeZip.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[24]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./src/reader.js", 20);\r\n      observeImports(map, "./src/types.js", 18);\r\n      observeImports(map, "./src/writer.js", 23);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[25]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./compartment-map.js", 1);\r\n      observeImports(map, "./json.js", 2);\r\n      observeImports(map, "./link.js", 6);\r\n      observeImports(map, "./node-module-specifier.js", 4);\r\n      observeImports(map, "./parse-bytes.js", 7);\r\n      observeImports(map, "./parse-json.js", 8);\r\n      observeImports(map, "./parse-pre-cjs.js", 10);\r\n      observeImports(map, "./parse-pre-mjs.js", 11);\r\n      observeImports(map, "./parse-text.js", 12);\r\n      observeImports(map, "./powers.js", 13);\r\n      observeImports(map, "@endo/zip", 24);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      makeArchiveImportHookMaker: cells[25].makeArchiveImportHookMaker.set,\r\n      parseArchive: cells[25].parseArchive.set,\r\n      loadArchive: cells[25].loadArchive.set,\r\n      importArchive: cells[25].importArchive.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n  functors[26]({\r\n    imports(entries) {\r\n      const map = new Map(entries);\r\n      observeImports(map, "./import-archive.js", 25);\r\n      observeImports(map, "./link.js", 6);\r\n    },\r\n    liveVar: {\r\n    },\r\n    onceVar: {\r\n      loadApplication: cells[26].loadApplication.set,\r\n    },\r\n    importMeta: {},\r\n  });\r\n\r\n  return cells[cells.length - 1]['*'].get();\r\n})();\r\n\r\n  })();\r\n  // END BUNDLE RUNTIME ================================\r\n\r\n  globalThis.Endo = Object.freeze({\r\n    loadApplication,\r\n  })\r\n  \n  `,{id:n}=r,{processes:{[n]:i}}=(0,a.z)(),{url:f=""}=i||{},[h,g]=(r=>{const[e,n]=m(r),[t,o]=p((async()=>{if(e)return[".jsdos",".wsz",".zip"].includes((0,c.RT)(r))?(0,l.unzip)(e):(0,l.unarchive)(r,e)}),[r,e]);return[t,n||o]})(f);if(g)throw g;const y=h&&h["/compartment-map.json"],b=y&&u.decode(y),_=(0,s.useCallback)(((r,e)=>{if(void 0===h)return;if("object"!=typeof r.data)return;if("object"!=typeof r.data.msg)return;const n=r.data.msg;if("lookupModule"===n.type){const{location:t}=n.params,o=h[`/${t}`],a=o&&u.decode(o),i=JSON.parse(a);e({...r.data,msg:{...n,result:i}})}}),[h]);if(!f)return null;if(!b)return null;const w={libs:["https://npmfs.com/download/ses/0.18.4/dist/ses.cjs"],runtime:e,transformInputSource:r=>`\n        let nextId = 0;\n        const send = (msg) => {\n          const id = nextId++;\n          window.parent.postMessage({ id, msg }, '*')\n          return new Promise((resolve) => {\n            const listener = ({ data }) => {\n              if (data.id === id) {\n                if (msg === data.msg) console.log('send: msg === data.msg')\n                globalThis.removeEventListener('message', listener);\n                resolve(data.msg);\n              }\n            };\n            globalThis.addEventListener('message', listener);\n          })\n        };\n\n        const compartmentMap = ${b};\n        \n        const lookupModule = async (location) => {\n          console.log('inside lookupModule:', location)\n          const { result } = await send({ type: 'lookupModule', params: { location } })\n          console.log('inside lookupModule result:', result)\n          return result\n        }\n        const archiveLocation = '(archive location)'\n\n        console.log('loading...')\n        const { execute } = Endo.loadApplication(\n          compartmentMap,\n          lookupModule,\n          archiveLocation,\n        )\n        console.log('executing...')\n        execute();\n      `};return(0,t.jsx)(o.default,{...r,onMessage:_,runtimeConfig:w})}},59824:function(r,e,n){n.r(e),n.d(e,{default:function(){return g},defaultRuntimeConfig:function(){return h}});var t=n(85893),o=n(26724),a=n(56758),i=n(25804),s=n(19682),c=n(67294),l=n(30663),p=n(34254);var m=n(80807);const d=()=>(0,t.jsx)("svg",{viewBox:"0 0 24 24",xmlns:"http://www.w3.org/2000/svg",children:(0,t.jsx)("path",{d:"M17.65 6.35A7.958 7.958 0 0 0 12 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08A5.99 5.99 0 0 1 12 18c-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"})}),u=()=>(0,t.jsx)("svg",{viewBox:"0 0 24 24",xmlns:"http://www.w3.org/2000/svg",children:(0,t.jsx)("path",{d:"M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"})}),f={referrerPolicy:"no-referrer",sandbox:"allow-same-origin allow-scripts"},h={libs:[],runtime:"{\n  const terminal = document.createElement('pre');\n  document.body.appendChild(terminal);\n  const _console = { ...console };\n  const createLogger = (type) => {\n    if (!(type in console)) {\n      throw new Error(`Invalid console type: ${type}`);\n    }\n    return (...args) => {\n      _console[type](...args);\n      terminal.innerText += `${type}: ${args.join()}\n`;\n    }\n  }\n  console.log = createLogger('log');\n  console.warn = createLogger('warn');\n  console.error = createLogger('error');\n  console.info = createLogger('info');\n  console.debug = createLogger('debug');\n  console.dir = createLogger('dir');\n}",transformInputSource:r=>r};var g=r=>{let{id:e,runtimeConfig:n=h,onMessage:g}=r;(r=>{const e=(0,c.useCallback)((e=>{const{origin:n,source:t}=e;"function"==typeof r&&r(e,(r=>{!function(r,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:"*";e?.postMessage(r,{targetOrigin:n})}(r,t,n)}))}),[r]);(0,c.useEffect)((()=>(globalThis.addEventListener("message",e),()=>globalThis.removeEventListener("message",e))),[e])})(g);const{icon:y,processes:{[e]:b}}=(0,a.z)(),_=b.url||"",{history:w,position:v}=(0,s.Z)(_,e),{exists:x,readFile:k}=(0,o.o)(),$=(0,c.useRef)(null),[E,A]=(0,c.useState)(!1),[R,M]=(0,c.useState)(""),[P,T]=(0,c.useState)(""),C=(0,c.useCallback)((async r=>{const{contentWindow:t}=$.current||{};if(t?.location){const t=await x(r);if(A(!0),M(""),t){const e=(await k(r)).toString(),t=n.transformInputSource(e),o=n.runtime,a=Array.prototype.map.call(n.libs,(r=>`<script src="${r}"><\/script>`)).join("\n");M(`\n          <html>\n            <head>\n            ${a}\n            </head>\n            <body></body>\n            <script>${o}<\/script>\n            <script>${t}<\/script>\n          </html>`)}y(e,i.Z.Browser.icon)}}),[x,e,k,y,n]),D=(0,c.useMemo)((()=>({backgroundColor:R?"#fff":"initial"})),[R]);return(0,c.useEffect)((()=>{b&&w[v]!==P&&(C(w[v]),T(w[v]))}),[P,w,v,b,C]),(0,t.jsxs)(m.Z,{$hasSrcDoc:Boolean(R),children:[(0,t.jsx)("nav",{children:(0,t.jsx)("div",{children:(0,t.jsx)(l.Z,{disabled:E,onClick:()=>C(w[v]),...(0,p.PS)("Reload this page"),children:E?(0,t.jsx)(u,{}):(0,t.jsx)(d,{})})})}),(0,t.jsx)("iframe",{ref:$,onLoad:()=>A(!1),srcDoc:R||void 0,style:D,title:e,...f})]})}},19682:function(r,e,n){var t=n(56758),o=n(67294);e.Z=(r,e)=>{const{url:n}=(0,t.z)(),[a,i]=(0,o.useState)(r),[s,c]=(0,o.useState)((()=>[r])),[l,p]=(0,o.useState)(0),m=(0,o.useCallback)((r=>{const t=l+r;p(t),i(s[t]),n(e,s[t])}),[n,s,e,l]);return(0,o.useEffect)((()=>{r!==a&&(p(l+1),i(r),c([...s.slice(0,l+1),r]))}),[a,s,l,r]),{canGoBack:l>0,canGoForward:l<s.length-1,currentUrl:a,history:s,moveHistory:m,position:l}}},91264:function(r,e,n){n.r(e),n.d(e,{addEntryToZippable:function(){return c},addFileToZip:function(){return m},createZippable:function(){return s},isFileInZip:function(){return d},unarchive:function(){return u},unzip:function(){return l},zipAsync:function(){return p}});var t=n(1864),o=n(64358),a=n(34254),i=n(48764).Buffer;const s=(r,e)=>r.split("/").reduceRight(((r,e)=>({[e]:r})),[e,o.BF]),c=(r,e)=>{const[[n,t]]=Object.entries(e);return r[n]=n in r?c(r[n],e[n]):t,r},l=r=>new Promise(((e,t)=>{n.e(3778).then(n.bind(n,93778)).then((n=>{let{unzip:o}=n;return o(r,((r,n)=>r?t(r):e(n)))}))})),p=function(r){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:o.BF;return new Promise(((t,o)=>{n.e(3778).then(n.bind(n,93778)).then((n=>{let{zip:a}=n;return a(r,e,((r,e)=>r?o(r):t(e)))}))}))},m=async(r,e,n,t)=>i.from(await p(c(r.length>0&&await l(r)||{},s(n,await t(e))))),d=(r,e)=>new Promise(((t,o)=>{n.e(3778).then(n.bind(n,93778)).then((n=>{let{unzip:a}=n;return a(r,((r,n)=>r?o(r):t(Object.keys(n).includes(e))))}))})),u=async(r,e)=>{if(window.SevenZip||await(0,a.mb)(["System/7zip/7zz.es6.js"]),!window.SevenZip)return{};const n=await window.SevenZip(),o=(0,t.basename)(r),s=(0,t.join)("/",(0,t.basename)(r,(0,t.extname)(r)));n.FS.mkdir(s),n.FS.chdir(s);const c=n.FS.open(o,"w+");n.FS.write(c,e,0,e.length),n.FS.close(c),n.callMain(["-y","x",o]);const l=n.FS.readdir(s),p=r=>(e,a)=>{if([".","..",o].includes(a))return e;const c=(0,t.join)(r,a),l=c.replace(s,"");try{n.FS.chmod(c,511)}catch{}return Object.assign(e,n.FS.isDir(n.FS.stat(c).mode)?{[(0,t.join)(l,"/")]:i.from(""),...n.FS.readdir(c).reduce(p(c),{})}:{[l]:n.FS.readFile(c,{flags:"r"})}),e};return l.reduce(p(s),{})}}}]);