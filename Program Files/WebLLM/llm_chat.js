class Conversation{constructor(t){this.system=t.system,this.roles=t.roles,this.offset=t.offset,this.seps=t.seps,this.convId=null,this.messages=[],this.contextWindowStart=0}getPromptArray(){if(0==this.seps.length)throw Error("Need seps to work");let t=[this.system+this.seps[0]];for(let e=0;e<this.messages.length;++e){const s=this.messages[e],i=s[0],n=s[1];void 0!==n&&""!=n?t.push(i+": "+n+this.seps[e%this.seps.length]):t.push(i+":")}return t}getPromptArrayUnproccessed(){if(0==this.seps.length)throw Error("Need seps to work");if(this.messages.length<3)throw Error("needs to call getLastPromptArray for the first message");let t=[this.seps[this.seps.length-1]];for(let e=this.messages.length-2;e<this.messages.length;++e){const s=this.messages[e],i=s[0],n=s[1];void 0!==n&&""!=n?t.push(i+": "+n+this.seps[e%this.seps.length]):t.push(i+":")}return t}getLastPromptArray(){if(0==this.seps.length)throw Error("Need seps to work");let t=[this.system+this.seps[0]];for(let e=this.messages.length-2;e<this.messages.length;++e){const s=this.messages[e],i=s[0],n=s[1];void 0!==n&&""!=n?t.push(i+": "+n+this.seps[e%this.seps.length]):t.push(i+":")}return t}reset(){this.messages=[]}getStopStr(){return this.seps[this.seps.length-1]}appendMessage(t,e){this.messages.push([t,e])}}function defaultConversation(t=2048){return new Conversation({system:globalThis.tvmjsGlobalEnv.systemPrompt||"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.",roles:["USER","ASSISTANT"],maxWindowLength:t,messages:[],offset:0,seps:[" ","</s>"]})}class LLMChatPipeline{constructor(t,e,s,i){if(null==s)throw Error("Expect cacheMetadata");this.tvm=t,this.logger=globalThis.tvmjsGlobalEnv.logger||console.log,this.tokenizer=e,this.bosTokenId=1,this.eosTokenId=2,this.maxWindowLength=i.maxWindowLength,this.maxGenLength=i.maxGenLength,this.meanGenLength=i.meanGenLength,this.streamInterval=1,this.decodingTotalTime=0,this.decodingTotalTokens=0,this.encodingTotalTime=0,this.encodingTotalTokens=0,this.conversation=defaultConversation(this.maxWindowLength),this.device=this.tvm.webgpu(),this.vm=this.tvm.detachFromCurrentScope(this.tvm.createVirtualMachine(this.device)),this.encoding=this.tvm.detachFromCurrentScope(this.vm.getFunction("encoding")),this.decoding=this.tvm.detachFromCurrentScope(this.vm.getFunction("decoding")),this.params=this.tvm.detachFromCurrentScope(this.tvm.getParamsFromCache("param",s.ParamSize));const n=this.vm.getFunction("create_kv_cache");this.fclearKVCaches=this.tvm.detachFromCurrentScope(this.tvm.getGlobalFunc("vm.builtin.attention_kv_cache_array_clear")),this.kvCache=this.tvm.detachFromCurrentScope(n()),this.logitsOnCPU=void 0,this.kvCacheLength=0,this.clearCache=!0}dispose(){this.params.dispose(),this.decoding.dispose(),this.encoding.dispose(),this.vm.dispose(),this.kvCache.dispose(),this.fclearKVCaches.dispose(),null!=this.logitsOnCPU&&this.logitsOnCPU.dispose()}#t(){this.fclearKVCaches(this.kvCache)}#e(t,e){var s;this.tvm.beginScope();const i=this.tvm.makeShapeTuple([e]);s=t.shape[1]>1?this.encoding(t,i,this.kvCache,this.params):this.decoding(t,i,this.kvCache,this.params);const n=this.tvm.detachFromCurrentScope(s.get(0));return this.tvm.endScope(),this.tvm.attachToCurrentScope(n),n}#s(t){if(null==this.logitsOnCPU)this.logitsOnCPU=this.tvm.detachFromCurrentScope(this.tvm.empty(t.shape,t.dtype,this.tvm.cpu()));else if(t.shape[0]!=this.logitsOnCPU.shape[0])throw Error("We expect the size of logits to remain unchanged");this.logitsOnCPU.copyFrom(t)}async sampleTokenFromLogits(t,e=.8,s=.95){return this.tvm.beginScope(),this.#s(t),this.tvm.endScope(),await this.device.sync(),this.tvm.sampleTopPFromLogits(this.logitsOnCPU,e,s)}async getInputTokens(){let t=[this.bosTokenId],e="";this.conversation.messages.length<=2?e=this.conversation.getPromptArray():(t.pop(),e=this.conversation.getPromptArrayUnproccessed()),t.push(...await this.tokenizer.encodeIds(e[0]));let s=t.length,i=[],n=!1;for(let t=e.length-1;t>0;--t){const o=this.tokenizer.encodeIds(e[t]);if(s+=o.length,this.kvCacheLength+s+this.meanGenLength>=this.maxWindowLength){n=!0;break}i.unshift(o)}if(!n){for(const e of i)t.push(...e);return t}this.kvCacheLength=0,this.clearCache=!0,t=[this.bosTokenId];let o=this.conversation.getPromptArray();t.push(...await this.tokenizer.encodeIds(o[0])),i=[],s=t.length;for(let t=o.length-1;t>0;--t){const e=this.tokenizer.encodeIds(o[t]);if(s+=e.length,s>=.1*this.maxWindowLength&&t+2<o.length)break;i.unshift(e)}for(const e of i)t.push(...e);if(t.length+this.meanGenLength>=this.maxWindowLength)throw Error("Exceed max window length curr="+t.length);return t}resetChat(){this.conversation.reset(),this.#t(),this.decodingTotalTime=0,this.encodingTotalTime=0,this.decodingTotalTokens=0,this.encodingTotalTokens=0}async generate(t,e){this.conversation.appendMessage(this.conversation.roles[0],t),this.conversation.appendMessage(this.conversation.roles[1],"");const s=this.conversation.getStopStr(),i=await this.getInputTokens(),n=i.length;var o="";this.clearCache&&(this.#t(),this.clearCache=!1);const a=Math.min(this.maxGenLength,this.maxWindowLength-i.length);if(a<this.meanGenLength)throw Error("Too small window size config");let h=0;for(;h<a&&this.kvCacheLength+n+h<this.maxWindowLength;++h){var r;this.tvm.beginScope();let t=performance.now();0==h?(r=this.tvm.empty([1,i.length],"int32",this.device)).copyFrom(i):(r=this.tvm.empty([1,1],"int32",this.device)).copyFrom(i.slice(i.length-1));const a=this.tvm.detachFromCurrentScope(this.#e(r,this.kvCacheLength+n+h));this.tvm.endScope();const c=await this.sampleTokenFromLogits(a);a.dispose(),i.push(c);const g=i.slice(n);if(o=this.tokenizer.decodeIds(g),c==this.eosTokenId)break;const l=o.lastIndexOf(s);if(-1!=l){o=o.substring(0,l);break}let d=performance.now();0!=h?(this.decodingTotalTokens+=1,this.decodingTotalTime+=(d-t)/1e3):(this.encodingTotalTime+=(d-t)/1e3,this.encodingTotalTokens+=n),h%this.streamInterval==0&&e(h,o)}return this.kvCacheLength+=i.length-1,this.conversation.messages[this.conversation.messages.length-1][1]=o,o}async evaluate(){this.#t();const t=await this.tokenizer.encodeIds("The capital of Canada is"),e=(t.length,Array.from(t));if(e.unshift(this.bosTokenId),0==e.length)throw Error("empty token");this.tvm.beginScope();const s=this.tvm.empty([1,e.length],"int32",this.device);s.copyFrom(e);const i=performance.now();this.#e(s,e.length),this.tvm.endScope(),await this.device.sync();const n=performance.now();this.tvm.beginScope();const o=this.tvm.empty([1,1],"int32",this.device).copyFrom([6234]);this.#s(this.#e(o,e.length+1)),await this.device.sync(),this.tvm.endScope();const a=performance.now(),h=`encoding-time=${((n-i)/1e3).toFixed(4)} secdecoding-time=${((a-n)/1e3).toFixed(4)} sec`;console.log("Logits:"),console.log(this.logitsOnCPU.toArray()),console.log(h)}async asyncLoadWebGPUPiplines(){await this.tvm.asyncLoadWebGPUPiplines(this.vm.getInternalModule())}runtimeStatsText(){return`encoding: ${(this.encodingTotalTokens/this.encodingTotalTime).toFixed(4)} tokens/sec, decoding: ${(this.decodingTotalTokens/this.decodingTotalTime).toFixed(4)} tokens/sec`}}class LLMChatInstance{constructor(){this.requestInProgress=!1,this.config=void 0,this.tvm=void 0,this.pipeline=void 0,this.logger=globalThis.tvmjsGlobalEnv.logger||console.log,this.debugTest=!1}async#i(t,e){if(void 0!==this.tvm)return;this.logger=globalThis.tvmjsGlobalEnv.logger||console.log;const s=await(await fetch(t)).arrayBuffer(),i=await tvmjs.instantiate(new Uint8Array(s),new EmccWASI,this.logger);try{const t=await tvmjs.detectGPUDevice();if(void 0===t)throw this.appendMessage("error","This browser env do not support WebGPU"),this.reset(),Error("This browser env do not support WebGPU");var n="WebGPU";0!=t.adapterInfo.description.length?n+=" - "+t.adapterInfo.description:n+=" - "+t.adapterInfo.vendor,this.appendMessage("init","Initialize GPU device: "+n),i.initWebGPU(t.device)}catch(t){throw this.appendMessage("error","Find an error initializing the WebGPU device "+t.toString()),this.logger("[error]",t.toString()),console.log(t),this.reset(),Error("Find an error initializing WebGPU: "+t.toString())}this.tvm=i;i.registerInitProgressCallback((t=>{this.updateLastMessage("init",t.text)})),await i.fetchNDArrayCache(e,i.webgpu())}async asyncInit(){void 0===this.pipeline&&(await this.#n(),await this.#i(this.config.wasmUrl,this.config.cacheUrl),await this.#o())}async#n(){void 0===this.config&&(this.config=await(await fetch("/Program Files/WebLLM/config.json")).json())}async#o(){if(void 0!==this.pipeline)return;const t=await tvmjsGlobalEnv.sentencePieceProcessor(this.config.tokenizer);this.pipeline=this.tvm.withNewScope((()=>new LLMChatPipeline(this.tvm,t,this.tvm.cacheMetadata,this.config))),await this.pipeline.asyncLoadWebGPUPiplines(),this.updateLastMessage("init","All initialization finished.")}appendMessage(t,e){"init"==t&&(e="[System Initalize] "+e),this.logger(`[${t}]`,e)}updateLastMessage(t,e){"init"==t?this.logger("[init]",e):"left"==t&&(globalThis.tvmjsGlobalEnv.response=e)}async respondTestMessage(t){const e=await this.pipeline.tokenizer.encodeIds("I am a friendly bot. Please ask questions."),s=[];for(let i=0;i<t;++i)for(let t=0;t<e.length;++t){s.push(e[t]);const i=this.pipeline.tokenizer.decodeIds(s);this.updateLastMessage("left",i),await new Promise((t=>setTimeout(t,50)))}}resetChat(){this.pipeline.resetChat()}async generate(){if(this.requestInProgress)return;this.requestInProgress=!0;try{await this.asyncInit()}catch(t){return this.appendMessage("error","Init error, "+t.toString()),this.logger("[error]",t.toString()),console.log(t),this.reset(),void(this.requestInProgress=!1)}if(this.debugTest)return await this.pipeline.evaluate(),void(this.requestInProgress=!1);const t=globalThis.tvmjsGlobalEnv.message;if(""==t)return void(this.requestInProgress=!1);this.appendMessage("progress","Generating...");const e=(t,e)=>{e.endsWith("##")?e=e.substring(0,e.length-2):e.endsWith("#")&&(e=e.substring(0,e.length-1)),this.updateLastMessage("left",e),this.appendMessage("progress",`Generating step: ${t}...`)};try{const s=await this.pipeline.generate(t,e);this.updateLastMessage("left",s)}catch(t){this.appendMessage("error","Generate error, "+t.toString()),this.logger("[error]",t.toString()),console.log(t),this.reset()}this.requestInProgress=!1}reset(){this.tvm=void 0,void 0!==this.pipeline&&this.pipeline.dispose(),this.pipeline=void 0}}localLLMChatIntance=new LLMChatInstance,tvmjsGlobalEnv.asyncOnGenerate=async function(){await localLLMChatIntance.generate()},tvmjsGlobalEnv.asyncOnReset=async function(){await localLLMChatIntance.resetChat()};